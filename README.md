# Visual Description (VD) Pipeline

This project build a VD pipeline to describe a scene (from an image) with the hope that it could guide visually impaired people.

To this end, we sequentially use a high level image-to-text model (LLaVA-Next aka LLaVA-1.6 7B), followed by a open-label segmentation model (Grounding DINO) and finally a text-to-text model (Phi3-mini-VD) to build a scene description paragraph.

LLaVA is prompted with the image and some instructions to generate a scene title and several labels of objects or entities which are in the image. The labels are then used as inputs to Grounding DINO which outputs bounding boxes for those. Phi3-mini produces a descriptive text out of the title, labels and boxes coordinates.

Extensive post-processing on the bounding boxes is applied before being fed to the LLM. Among else, it merges semantically too close labels (we use an embedding model for this: gte-large-en-v1.5), merges overlapping boxes, removes least coherent boxes when similar to another, ... In addition, we generate multiple [title/labels] simultaneously via batching and feed them all to Grounding DINO to improve consistency of the pipeline.

The original pipeline did not leverage any fine-tuning but uses LLaMA-3 70B (GGUF quantized 4bits) to produce the the final descriptive text as a powerful model is needed for this step (see app_vanilla_VDpipeline.py). The second version is the result of a Phi3-mini fine-tuning for this task (Phi3-mini-VD) and can enjoy a lighter pipeline with similar performance (see app_finetuned_VDpipeline.py).

## The Fine-tuning

The FT dataset is built using the original pipeline. We use the COCO dataset (2014) as image bank. We produced 12k training images and 12k validation images (in 4 days on one A100). The dataset corresponds to a json file containing for each image: the title (generated by LLaVA using a controlled generation to ensure parse-ability of the output), the bounding boxes (produced by Grounding DINO) and the descriptive text (from LLaMA-3).

The actual fine-tuning is performed on Phi3-mini with LoRA applied to all attention matrices (3.8M trainable params ie. 0.1% of total parameters). It takes ~30mn on one A100.