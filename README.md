# Visual Description (VD) Pipeline

This project build a VD pipeline to describe a scene (from an image) with the hope that it could guide visually impaired people.

To this end, we sequentially use a high level image-to-text model (LLaVA-Next aka LLaVA-1.6 7B), followed by a open-label segmentation model (Grounding DINO) and finally a text-to-text model (Phi3-mini-VD) to build a scene description paragraph.

LLaVA is prompted with the image and some instructions to generate a scene title and several labels of objects or entities which are in the image. The labels are then used as inputs to Grounding DINO which outputs bounding boxes for those. Phi3-mini produces a descriptive text out of the title, labels and boxes coordinates.

Extensive post-processing on the bounding boxes is applied before being fed to the LLM. Among else, it merges semantically too close labels (we use an embedding model for this: gte-large-en-v1.5), merges overlapping boxes, removes least coherent boxes when similar to another, ... In addition, we generate multiple [title/labels] simultaneously via batching and feed them all to Grounding DINO to improve consistency of the pipeline.

The original pipeline did not leverage any fine-tuning but uses LLaMA-3 70B (GGUF quantized 4bits) to produce the the final descriptive text as a powerful model is needed for this step (see app_vanilla_VDpipeline.py). The second version is the result of a Phi3-mini fine-tuning for this task (Phi3-mini-VD) and can enjoy a lighter pipeline with similar performance (see app_finetuned_VDpipeline.py).

## Fine-tuning

The FT dataset is built using the original pipeline. We use the COCO dataset (2014) as image bank. We produced 12k training images and 12k validation images (in 4 days on one A100). The dataset corresponds to a json file containing for each image: the title (generated by LLaVA using a controlled generation to ensure parse-ability of the output), the bounding boxes (produced by Grounding DINO) and the descriptive text (from LLaMA-3).

The actual fine-tuning is performed on Phi3-mini with LoRA applied to all attention matrices for 20k steps (3.8M trainable params ie. 0.1% of total parameters). It takes ~1h on one A100.

## Fine-tuning 2

Alternatively, we also try to fine-tune llava (LLaVA-v1.6-vicuna-7b) directly (the llm) with the generated dataset (see finetune_task_lora.sh and train2014_llava.json). In this case, we don't need the bounding boxes or labels, as we only feed the images along with the descriptive text. (Note: we slightly modified the train.py file from LLaVA repository to evaluate the model during training on the eval dataset and allow for apple-to-apple comparison of our models.)

We again applied LoRA to all attention matrices for 20k steps. It takes ~5h on one A100. (We keep the 12k checkpoint as beyond this we observe some overfitting.)

After analyzing the results, the whole pipeline with a fine-tuned Phi3-mini performs consistently better than just using a fine-tuned LLaVA (eval_loss=0.52 vs. 0.87). Some result examples are available in the table output_examples/Visual_Description_Fine-tuned_Pipeline_Examples.pdf .

![Segmentation with Grounding DINO](https://github.com/sade-adrien/visual_description/blob/master/data/output_examples/output_image.jpg)

