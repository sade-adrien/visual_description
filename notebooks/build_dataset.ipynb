{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer, AutoModel, AutoModelForCausalLM, LlavaNextForConditionalGeneration, AutoModelForZeroShotObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageColor, ImageFont\n",
    "from llama_cpp import Llama\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import json\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "image_to_text_model = 'llava-hf/llava-v1.6-mistral-7b-hf'\n",
    "segmentation_model = \"IDEA-Research/grounding-dino-base\"\n",
    "embeddings_model = 'Alibaba-NLP/gte-large-en-v1.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    processor_itt = AutoProcessor.from_pretrained(image_to_text_model)\n",
    "    model_itt = LlavaNextForConditionalGeneration.from_pretrained(image_to_text_model,\n",
    "                                                low_cpu_mem_usage = True,\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                device_map=device,\n",
    "                                                cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                                )\n",
    "\n",
    "    processor_segmentation = AutoProcessor.from_pretrained(segmentation_model)\n",
    "    model_segmentation = AutoModelForZeroShotObjectDetection.from_pretrained(segmentation_model,\n",
    "                                                                    low_cpu_mem_usage = True,\n",
    "                                                                    device_map=device,\n",
    "                                                                    cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                                                    )\n",
    "\n",
    "    tokenizer_embeddings = AutoTokenizer.from_pretrained(embeddings_model)\n",
    "    model_embeddings = AutoModel.from_pretrained(embeddings_model,\n",
    "                                        low_cpu_mem_usage = True,\n",
    "                                        torch_dtype=torch.float16,\n",
    "                                        device_map=device,\n",
    "                                        trust_remote_code=True,\n",
    "                                        cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                        )\n",
    "                        \n",
    "    return model_itt, processor_itt, model_segmentation, processor_segmentation, model_embeddings, tokenizer_embeddings\n",
    "\n",
    "def get_labels_color(labels):\n",
    "    all_colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown', 'gray', 'cyan', 'magenta', 'lime', 'teal', 'navy', 'maroon', 'olive', 'indigo', 'violet', 'coral', 'salmon', 'gold', 'silver', 'turquoise', 'lavender', 'beige', 'tan', 'mint', 'plum', 'khaki', 'ivory', 'honeydew']\n",
    "    labels = list(set(labels))\n",
    "\n",
    "    labels_to_colors = [[label, color] for label, color in zip(labels, all_colors)]\n",
    "\n",
    "    return dict(labels_to_colors)\n",
    "\n",
    "def save_image_with_boxes(image, segments, path='./output_image.jpg'):\n",
    "    boxes, labels, scores = segments[0]['boxes'], segments[0]['labels'], segments[0]['scores']\n",
    "    \n",
    "    labels_to_colors = get_labels_color(labels)\n",
    "\n",
    "    img = image.copy().convert(\"RGBA\")\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "\n",
    "        box_width = max(1, int(0.0035 * max(img.size)))\n",
    "        text_size = max(9, int(0.01 * max(img.size)))\n",
    "\n",
    "\n",
    "        overlay = Image.new('RGBA', img.size, (0,0,0,0))\n",
    "        draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "        transparency = 200\n",
    "        box = box.cpu().tolist()\n",
    "        outline_color = ImageColor.getcolor(labels_to_colors[label], \"RGB\")\n",
    "        draw.rectangle(box, outline=(outline_color[0], outline_color[1], outline_color[2], transparency), width=box_width)\n",
    "\n",
    "        font = ImageFont.load_default()\n",
    "        font = font.font_variant(size=text_size)\n",
    "\n",
    "        position = (box[0] + box_width, box[1] + box_width)\n",
    "        text_bbox = draw.textbbox(position, label, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0] + 2\n",
    "        text_height = text_bbox[3] - text_bbox[1] + 2\n",
    "        background_rectangle = [position[0], position[1], position[0] + text_width, position[1] + text_height]\n",
    "        background_rectangle = [val + 2 if i%2==1 else val for i, val in enumerate(background_rectangle)] #add some margin to fit the text position\n",
    "\n",
    "        draw.rectangle(background_rectangle, fill='grey')\n",
    "        draw.text(position, label, fill='white', font=font)\n",
    "\n",
    "        img = Image.alpha_composite(img, overlay)\n",
    "\n",
    "    img = img.convert(\"RGB\")\n",
    "    img.save(path)\n",
    "    return path\n",
    "\n",
    "def get_labels(model, processor, image, n_objects=10, n_parallel_inference=3):\n",
    "    generation_success = False\n",
    "    while not generation_success:\n",
    "        outputs = controled_generation(model, processor, [image] * n_parallel_inference, n_objects=n_objects, do_sample=True, temperature=0.7)\n",
    "        try:\n",
    "            list_outputs = []\n",
    "            for output in outputs:\n",
    "                list_outputs.append(ast.literal_eval(output))\n",
    "            # if len(list_outputs[-1]['objects']) <= n_objects*1.2 and len(list_outputs[-1]['objects']) >= n_objects*0.8:\n",
    "            generation_success = True\n",
    "            # else:\n",
    "            #     print(f\"Failed to generate the right number of labels: {len(list_outputs[-1]['objects'])}\")\n",
    "\n",
    "        except:\n",
    "            print(f\"An error occured during json evaluation of generated output: {outputs}\")\n",
    "            continue\n",
    "    \n",
    "    list_labels = ['. '.join(list_outputs[i]['objects']).lower() +'.' for i in range(n_parallel_inference)]\n",
    "    #### check if a list of titles can be usefull\n",
    "    title = list_outputs[0]['title']\n",
    "\n",
    "    return list_labels, title\n",
    "    \n",
    "def run_segmentation(model, processor, image, list_labels):\n",
    "    n_parallel = len(list_labels)\n",
    "    inputs = processor(images=[image] * n_parallel, text=list_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(outputs,\n",
    "                                                            inputs.input_ids,\n",
    "                                                            box_threshold=0.2,\n",
    "                                                            text_threshold=0.2,\n",
    "                                                            target_sizes=[image.size[::-1]] * n_parallel\n",
    "                                                            )\n",
    "\n",
    "    results = [{\n",
    "                'scores': torch.concatenate([results[i]['scores'] for i in range(n_parallel)]),\n",
    "                'labels': [l for sublist in [results[i]['labels'] for i in range(n_parallel)] for l in sublist],\n",
    "                'boxes': torch.concatenate([results[i]['boxes'] for i in range(n_parallel)])\n",
    "            }]\n",
    "\n",
    "    return results\n",
    "\n",
    "def compute_overlapping_area(boxes):\n",
    "    areas = np.zeros((boxes.shape[0], boxes.shape[0]))\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "        for j in range(boxes.shape[0]):\n",
    "            x0 = max(boxes[i, 0], boxes[j, 0])\n",
    "            y0 = max(boxes[i, 1], boxes[j, 1])\n",
    "            x1 = min(boxes[i, 2], boxes[j, 2])\n",
    "            y1 = min(boxes[i, 3], boxes[j, 3])\n",
    "\n",
    "            width = max(0, x1 - x0)\n",
    "            height = max(0, y1 - y0)\n",
    "\n",
    "            base_box_area = max((boxes[i, 2] - boxes[i, 0]) * (boxes[i, 3] - boxes[i, 1]), 1e-5)\n",
    "            intersection_area = min(width * height / base_box_area, 1)\n",
    "\n",
    "            areas[i,j] = intersection_area\n",
    "    \n",
    "    return areas\n",
    "\n",
    "def get_overlapping_index(intersection_areas, current_label, segments, threshold=.7):\n",
    "    indices_to_remove = []\n",
    "    real_indices = [k for k, val in enumerate(segments[0]['labels']) if val==current_label]\n",
    "\n",
    "    for i in range(intersection_areas.shape[0]):\n",
    "        for j in range(intersection_areas.shape[1]):\n",
    "            if i == j or real_indices[i] in indices_to_remove or real_indices[j] in indices_to_remove:\n",
    "                continue\n",
    "            if intersection_areas[i,j] >= threshold:\n",
    "                indices_to_remove.append(real_indices[i])\n",
    "\n",
    "    return indices_to_remove\n",
    "\n",
    "def remove_boxes(segments, boxes_to_remove):\n",
    "    n = segments[0]['scores'].shape[0]\n",
    "    new_scores = torch.tensor([segments[0]['scores'].tolist()[i] for i in range(n) if i not in boxes_to_remove], device=device)\n",
    "    new_labels = [segments[0]['labels'][i] for i in range(n) if i not in boxes_to_remove]\n",
    "    new_boxes = torch.tensor([segments[0]['boxes'].tolist()[i] for i in range(n) if i not in boxes_to_remove], device=device)\n",
    "    \n",
    "    return [{'scores': new_scores, 'labels': new_labels, 'boxes': new_boxes}]\n",
    "\n",
    "def get_overlapping_boxes(segments):  #overlapping w/ same labels\n",
    "    unique_labels = list(set(segments[0]['labels']))\n",
    "    boxes_to_remove = []\n",
    "\n",
    "    for l in unique_labels:\n",
    "        indices = [i for i in range(len(segments[0]['labels'])) if segments[0]['labels'][i] == l]\n",
    "        boxes = np.array([segments[0]['boxes'][i].cpu() for i in indices])\n",
    "        intersection_areas = compute_overlapping_area(boxes)\n",
    "        boxes_to_remove.extend(get_overlapping_index(intersection_areas, l, segments))\n",
    "\n",
    "    boxes_to_remove = list(set(boxes_to_remove))\n",
    "    return boxes_to_remove\n",
    "\n",
    "def get_similar_boxes(segments, threshold=.75): #similar w/ different (or same) labels\n",
    "    boxes_to_remove = []\n",
    "\n",
    "    intersection_areas = compute_overlapping_area(segments[0]['boxes'].cpu())\n",
    "\n",
    "    for i in range(intersection_areas.shape[0]):\n",
    "        for j in range(i):\n",
    "            if intersection_areas[i,j] >= threshold and intersection_areas[j,i] >= threshold:\n",
    "                idx = i if segments[0]['scores'][i] < segments[0]['scores'][j] else j\n",
    "                boxes_to_remove.append(idx)\n",
    "\n",
    "    boxes_to_remove = list(set(boxes_to_remove))\n",
    "    return boxes_to_remove\n",
    "\n",
    "def merge_labels(model, tokenizer, segments, display_similarity=False, threshold=.85):\n",
    "    real_labels = list(set(segments[0]['labels']))\n",
    "    inputs = tokenizer(real_labels, max_length=8192, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state[:, 0]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    scores = (embeddings @ embeddings.T)\n",
    "\n",
    "    similar_pairs = {}\n",
    "    for i in range(scores.shape[0]):\n",
    "        for j in range(i):\n",
    "            if display_similarity:\n",
    "                print(f\"similarity({real_labels[i], real_labels[j]})={scores[i,j]}\")\n",
    "            if scores[i,j] >= threshold:\n",
    "                label_to_be_replaced = real_labels[i] if len(real_labels[i]) >= len(real_labels[j]) else real_labels[j]\n",
    "                label_to_replace_with = real_labels[i] if label_to_be_replaced == real_labels[j] else real_labels[j]\n",
    "                similar_pairs[label_to_be_replaced] = label_to_replace_with\n",
    "\n",
    "    all_merged = False\n",
    "    while not all_merged:\n",
    "        all_merged = True\n",
    "        for k in range(len(segments[0]['labels'])):\n",
    "            if segments[0]['labels'][k] in similar_pairs.keys():\n",
    "                #print(f\"Merging label '{segments[0]['labels'][k]}' into '{similar_pairs[segments[0]['labels'][k]]}'...\")\n",
    "                segments[0]['labels'][k] = similar_pairs[segments[0]['labels'][k]]\n",
    "                all_merged = False\n",
    "\n",
    "    return segments\n",
    "    \n",
    "def controled_generation(model, processor, image, n_objects=10, **kwargs):\n",
    "    n_parallel = len(image)\n",
    "    prompt = f\"\"\"[INST] <image>\\nAnalyze the scene and infer what it is representing. Given the scene, list 5 to 10 objects or entities most likely to be part of the scene and most important to spot (use ONE SIMPLE WORD ONLY to describe an object or entity - this will be reprenting a category in large meaning). Ignore objects that are small or irrelevant to a blind person. Answer by filling out the following JSON format. Your answer must be parse-able with python's ast.literal_eval() - DO NOT ADD ANYTHING ELSE:\n",
    "    {{\n",
    "        \"title\": \"short_scene_title\",\n",
    "        \"objects\": {[f\"object_{i}\" for i in range(n_objects)]}\n",
    "    }}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    starter = \"\"\"   {\n",
    "        \"title\": \\\"\"\"\"\n",
    "\n",
    "    inputs = processor([prompt + starter] * n_parallel, image, return_tensors='pt').to(device, torch.float16)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, tokenizer=processor.tokenizer, stop_strings='\\\",', **kwargs)\n",
    "\n",
    "    intermediary = \"\"\"\n",
    "        \\\"objects\\\": ['\"\"\"\n",
    "\n",
    "    outputs = processor.tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    outputs = [re.sub(r'<s>|</s>|<pad>', '', o) + \"\\n    \\\"objects\\\": ['\" for o in outputs]\n",
    "\n",
    "    inputs['input_ids'], inputs['attention_mask'] = processor.tokenizer(outputs, return_tensors='pt', padding=True, add_special_tokens=False).to(device).values()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, tokenizer=processor.tokenizer, stop_strings=[\"]\", \"dummy string to circumvent bug in stop_strings\"], **kwargs)\n",
    "\n",
    "    ending = \"\"\"\n",
    "        }\"\"\"\n",
    "\n",
    "    outputs = processor.tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    outputs = [re.sub(r'<s>|</s>|<pad>', '', o)[len(prompt):] + \"\\n    }\" for o in outputs]\n",
    "    \n",
    "    return outputs\n",
    "    \n",
    "def post_processing(model_embeddings, tokenizer_embeddings, segments):\n",
    "    ## merging labels that are close in semantics\n",
    "    segments_merged_labels = merge_labels(model_embeddings, tokenizer_embeddings, segments, display_similarity=False)\n",
    "\n",
    "    ## removing boxes that are overlapping a lot with another same-label box\n",
    "    boxes_to_remove = get_overlapping_boxes(segments_merged_labels)\n",
    "    segments_sparser_boxes = remove_boxes(segments_merged_labels, boxes_to_remove)\n",
    "\n",
    "    ## removing boxes that are almost the same as an other but with a different label (keeping the one with highest score)\n",
    "    boxes_to_remove = get_similar_boxes(segments_sparser_boxes)\n",
    "    segments_cleaned = remove_boxes(segments_sparser_boxes, boxes_to_remove)\n",
    "\n",
    "    return segments_cleaned\n",
    "\n",
    "def run_image_description(model_llm, image, title, segments):\n",
    "    system_prompt = \"You are an AI model designed to help visually impaired people. Your task is to provide a comprehensive description of the image, locating important objects to guide disabled people through the scene.\"\n",
    "\n",
    "    width, height = image.size\n",
    "    segments[0]['boxes'][:, 0::2] /= width\n",
    "    segments[0]['boxes'][:, 1::2] /= height\n",
    "\n",
    "    boxes = [row for row in segments[0]['boxes'].tolist()]\n",
    "    boxes = [str([round(x,2) for x in row]) for row in boxes]\n",
    "\n",
    "    labels = segments[0]['labels']\n",
    "\n",
    "    box_prompt = '\\n'.join(sorted([a + ' ' + b for a,b in zip(labels, boxes)]))\n",
    "\n",
    "    question_prompt = f\"Below is a description of a {title} scene, along with a list of objects present in the scene along with their coordinates following the format 'object [x_min, y_min, x_max, y_max]'. Provide a descriptive paragraph using a human-like description, do not mention coordinates. Only use the position information and infer from it, do not add any comment or guess. Remain factual and avoid unnecessary embellishments, keep it simple.\"\n",
    "\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "    {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    {question_prompt}\n",
    "    {box_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = model_llm(\n",
    "       prompt,\n",
    "       max_tokens=500,\n",
    "       echo=True,\n",
    "   )\n",
    "\n",
    "    return outputs[\"choices\"][0][\"text\"].strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634c932b30e5432f9843c68abb4fba40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GroundingDinoForObjectDetection were not initialized from the model checkpoint at IDEA-Research/grounding-dino-base and are newly initialized: ['bbox_embed.1.layers.0.bias', 'bbox_embed.1.layers.0.weight', 'bbox_embed.1.layers.1.bias', 'bbox_embed.1.layers.1.weight', 'bbox_embed.1.layers.2.bias', 'bbox_embed.1.layers.2.weight', 'bbox_embed.2.layers.0.bias', 'bbox_embed.2.layers.0.weight', 'bbox_embed.2.layers.1.bias', 'bbox_embed.2.layers.1.weight', 'bbox_embed.2.layers.2.bias', 'bbox_embed.2.layers.2.weight', 'bbox_embed.3.layers.0.bias', 'bbox_embed.3.layers.0.weight', 'bbox_embed.3.layers.1.bias', 'bbox_embed.3.layers.1.weight', 'bbox_embed.3.layers.2.bias', 'bbox_embed.3.layers.2.weight', 'bbox_embed.4.layers.0.bias', 'bbox_embed.4.layers.0.weight', 'bbox_embed.4.layers.1.bias', 'bbox_embed.4.layers.1.weight', 'bbox_embed.4.layers.2.bias', 'bbox_embed.4.layers.2.weight', 'bbox_embed.5.layers.0.bias', 'bbox_embed.5.layers.0.weight', 'bbox_embed.5.layers.1.bias', 'bbox_embed.5.layers.1.weight', 'bbox_embed.5.layers.2.bias', 'bbox_embed.5.layers.2.weight', 'model.decoder.bbox_embed.1.layers.0.bias', 'model.decoder.bbox_embed.1.layers.0.weight', 'model.decoder.bbox_embed.1.layers.1.bias', 'model.decoder.bbox_embed.1.layers.1.weight', 'model.decoder.bbox_embed.1.layers.2.bias', 'model.decoder.bbox_embed.1.layers.2.weight', 'model.decoder.bbox_embed.2.layers.0.bias', 'model.decoder.bbox_embed.2.layers.0.weight', 'model.decoder.bbox_embed.2.layers.1.bias', 'model.decoder.bbox_embed.2.layers.1.weight', 'model.decoder.bbox_embed.2.layers.2.bias', 'model.decoder.bbox_embed.2.layers.2.weight', 'model.decoder.bbox_embed.3.layers.0.bias', 'model.decoder.bbox_embed.3.layers.0.weight', 'model.decoder.bbox_embed.3.layers.1.bias', 'model.decoder.bbox_embed.3.layers.1.weight', 'model.decoder.bbox_embed.3.layers.2.bias', 'model.decoder.bbox_embed.3.layers.2.weight', 'model.decoder.bbox_embed.4.layers.0.bias', 'model.decoder.bbox_embed.4.layers.0.weight', 'model.decoder.bbox_embed.4.layers.1.bias', 'model.decoder.bbox_embed.4.layers.1.weight', 'model.decoder.bbox_embed.4.layers.2.bias', 'model.decoder.bbox_embed.4.layers.2.weight', 'model.decoder.bbox_embed.5.layers.0.bias', 'model.decoder.bbox_embed.5.layers.0.weight', 'model.decoder.bbox_embed.5.layers.1.bias', 'model.decoder.bbox_embed.5.layers.1.weight', 'model.decoder.bbox_embed.5.layers.2.bias', 'model.decoder.bbox_embed.5.layers.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_itt, processor_itt, model_segmentation, processor_segmentation, model_embeddings, tokenizer_embeddings = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llm = Llama(model_path=\"../Models_Tests/Meta-Llama-3-70B-Instruct.Q2_K.gguf\",\n",
    "            n_ctx=4096,\n",
    "            n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "            n_gpu_layers=-1,         # The number of layers to offload to GPU, if you have GPU acceleration available, if -1 all layers are offloaded\n",
    "            verbose=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/train2014/'\n",
    "file_list = os.listdir(data_dir)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for i, filename in tqdm(enumerate(file_list)):\n",
    "    image = Image.open(data_dir + filename).convert('RGB')\n",
    "\n",
    "    list_labels, title = get_labels(model_itt, processor_itt, image, n_objects=7, n_parallel_inference=3)\n",
    "\n",
    "    segments = run_segmentation(model_segmentation, processor_segmentation, image, list_labels)\n",
    "\n",
    "    segments_postprocessed = post_processing(model_embeddings, tokenizer_embeddings, segments)\n",
    "\n",
    "    output_path = save_image_with_boxes(image, segments_postprocessed, data_dir + 'boxed/' + filename[:-4] + '_boxed.jpg')\n",
    "\n",
    "    descriptive_text = run_image_description(model_llm, image, title, segments_postprocessed)\n",
    "\n",
    "    segments[0]['boxes'] = segments[0]['boxes'].tolist()\n",
    "    segments[0]['scores'] = segments[0]['scores'].tolist()\n",
    "    segments_postprocessed[0]['boxes'] = segments_postprocessed[0]['boxes'].tolist()\n",
    "    segments_postprocessed[0]['scores'] = segments_postprocessed[0]['scores'].tolist()\n",
    "    outputs.append({\n",
    "        'idx': i,\n",
    "        'image_path': data_dir + filename,\n",
    "        'boxed_image_path': data_dir + 'boxed/' + filename[:-4] + '_boxed.jpg',\n",
    "        'title': title,\n",
    "        'original_segments': segments,\n",
    "        'segments_postprocessed': segments_postprocessed,\n",
    "        'generated_descriptive_text': descriptive_text,\n",
    "    })\n",
    "\n",
    "    print(outputs[-1]['generated_descriptive_text'])\n",
    "    break\n",
    "\n",
    "    with open(f'###TBD{data_dir[:-1]}_descriptive_texts.json', 'w') as file:\n",
    "        json.dump(outputs, file, indent=4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "as-llamacpppython",
   "language": "python",
   "name": "as-llamacpppython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
