{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pipeline VD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer, AutoModel, AutoModelForCausalLM, LlavaNextForConditionalGeneration, AutoModelForZeroShotObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageColor, ImageFont\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import json\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "image_to_text_model = 'llava-hf/llava-v1.6-mistral-7b-hf'\n",
    "segmentation_model = \"IDEA-Research/grounding-dino-base\"\n",
    "embeddings_model = 'Alibaba-NLP/gte-large-en-v1.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    processor_itt = AutoProcessor.from_pretrained(image_to_text_model)\n",
    "    model_itt = LlavaNextForConditionalGeneration.from_pretrained(image_to_text_model,\n",
    "                                                low_cpu_mem_usage = True,\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                device_map=device,\n",
    "                                                cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                                )\n",
    "\n",
    "    processor_segmentation = AutoProcessor.from_pretrained(segmentation_model)\n",
    "    model_segmentation = AutoModelForZeroShotObjectDetection.from_pretrained(segmentation_model,\n",
    "                                                                    low_cpu_mem_usage = True,\n",
    "                                                                    device_map=device,\n",
    "                                                                    cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                                                    )\n",
    "\n",
    "    tokenizer_embeddings = AutoTokenizer.from_pretrained(embeddings_model)\n",
    "    model_embeddings = AutoModel.from_pretrained(embeddings_model,\n",
    "                                        low_cpu_mem_usage = True,\n",
    "                                        torch_dtype=torch.float16,\n",
    "                                        device_map=device,\n",
    "                                        trust_remote_code=True,\n",
    "                                        cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                        )\n",
    "                        \n",
    "    return model_itt, processor_itt, model_segmentation, processor_segmentation, model_embeddings, tokenizer_embeddings\n",
    "\n",
    "def get_labels_color(labels):\n",
    "    all_colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown', 'gray', 'cyan', 'magenta', 'lime', 'teal', 'navy', 'maroon', 'olive', 'indigo', 'violet', 'coral', 'salmon', 'gold', 'silver', 'turquoise', 'lavender', 'beige', 'tan', 'mint', 'plum', 'khaki', 'ivory', 'honeydew']\n",
    "    labels = list(set(labels))\n",
    "\n",
    "    labels_to_colors = [[label, color] for label, color in zip(labels, all_colors)]\n",
    "\n",
    "    return dict(labels_to_colors)\n",
    "\n",
    "def save_image_with_boxes(image, segments, path='./output_image.jpg'):\n",
    "    boxes, labels, scores = segments[0]['boxes'], segments[0]['labels'], segments[0]['scores']\n",
    "    \n",
    "    labels_to_colors = get_labels_color(labels)\n",
    "\n",
    "    img = image.copy().convert(\"RGBA\")\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "\n",
    "        box_width = max(1, int(0.0035 * max(img.size)))\n",
    "        text_size = max(9, int(0.01 * max(img.size)))\n",
    "\n",
    "\n",
    "        overlay = Image.new('RGBA', img.size, (0,0,0,0))\n",
    "        draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "        transparency = 200\n",
    "        box = box.cpu().tolist()\n",
    "        outline_color = ImageColor.getcolor(labels_to_colors[label], \"RGB\")\n",
    "        draw.rectangle(box, outline=(outline_color[0], outline_color[1], outline_color[2], transparency), width=box_width)\n",
    "\n",
    "        font = ImageFont.load_default()\n",
    "        font = font.font_variant(size=text_size)\n",
    "\n",
    "        position = (box[0] + box_width, box[1] + box_width)\n",
    "        text_bbox = draw.textbbox(position, label, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0] + 2\n",
    "        text_height = text_bbox[3] - text_bbox[1] + 2\n",
    "        background_rectangle = [position[0], position[1], position[0] + text_width, position[1] + text_height]\n",
    "        background_rectangle = [val + 2 if i%2==1 else val for i, val in enumerate(background_rectangle)] #add some margin to fit the text position\n",
    "\n",
    "        draw.rectangle(background_rectangle, fill='grey')\n",
    "        draw.text(position, label, fill='white', font=font)\n",
    "\n",
    "        img = Image.alpha_composite(img, overlay)\n",
    "\n",
    "    img = img.convert(\"RGB\")\n",
    "    img.save(path)\n",
    "    return path\n",
    "\n",
    "def get_labels(model, processor, image, n_objects=10, n_parallel_inference=3):\n",
    "    generation_success = False\n",
    "    while not generation_success:\n",
    "        outputs = controled_generation(model, processor, [image] * n_parallel_inference, n_objects=n_objects, do_sample=True, temperature=0.7)\n",
    "        try:\n",
    "            list_outputs = []\n",
    "            for output in outputs:\n",
    "                list_outputs.append(ast.literal_eval(output))\n",
    "            # if len(list_outputs[-1]['objects']) <= n_objects*1.2 and len(list_outputs[-1]['objects']) >= n_objects*0.8:\n",
    "            generation_success = True\n",
    "            # else:\n",
    "            #     print(f\"Failed to generate the right number of labels: {len(list_outputs[-1]['objects'])}\")\n",
    "\n",
    "        except:\n",
    "            print(f\"An error occured during json evaluation of generated output: {outputs}\")\n",
    "            continue\n",
    "    \n",
    "    list_labels = ['. '.join(list_outputs[i]['objects']).lower() +'.' for i in range(n_parallel_inference)]\n",
    "    #### check if a list of titles can be usefull\n",
    "    title = list_outputs[0]['title']\n",
    "\n",
    "    return list_labels, title\n",
    "    \n",
    "def run_segmentation(model, processor, image, list_labels):\n",
    "    n_parallel = len(list_labels)\n",
    "    inputs = processor(images=[image] * n_parallel, text=list_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(outputs,\n",
    "                                                            inputs.input_ids,\n",
    "                                                            box_threshold=0.2,\n",
    "                                                            text_threshold=0.2,\n",
    "                                                            target_sizes=[image.size[::-1]] * n_parallel\n",
    "                                                            )\n",
    "\n",
    "    results = [{\n",
    "                'scores': torch.concatenate([results[i]['scores'] for i in range(n_parallel)]),\n",
    "                'labels': [l for sublist in [results[i]['labels'] for i in range(n_parallel)] for l in sublist],\n",
    "                'boxes': torch.concatenate([results[i]['boxes'] for i in range(n_parallel)])\n",
    "            }]\n",
    "\n",
    "    return results\n",
    "\n",
    "def compute_overlapping_area(boxes):\n",
    "    areas = np.zeros((boxes.shape[0], boxes.shape[0]))\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "        for j in range(boxes.shape[0]):\n",
    "            x0 = max(boxes[i, 0], boxes[j, 0])\n",
    "            y0 = max(boxes[i, 1], boxes[j, 1])\n",
    "            x1 = min(boxes[i, 2], boxes[j, 2])\n",
    "            y1 = min(boxes[i, 3], boxes[j, 3])\n",
    "\n",
    "            width = max(0, x1 - x0)\n",
    "            height = max(0, y1 - y0)\n",
    "\n",
    "            base_box_area = max((boxes[i, 2] - boxes[i, 0]) * (boxes[i, 3] - boxes[i, 1]), 1e-5)\n",
    "            intersection_area = min(width * height / base_box_area, 1)\n",
    "\n",
    "            areas[i,j] = intersection_area\n",
    "    \n",
    "    return areas\n",
    "\n",
    "def get_overlapping_index(intersection_areas, current_label, segments, threshold=.7):\n",
    "    indices_to_remove = []\n",
    "    real_indices = [k for k, val in enumerate(segments[0]['labels']) if val==current_label]\n",
    "\n",
    "    for i in range(intersection_areas.shape[0]):\n",
    "        for j in range(intersection_areas.shape[1]):\n",
    "            if i == j or real_indices[i] in indices_to_remove or real_indices[j] in indices_to_remove:\n",
    "                continue\n",
    "            if intersection_areas[i,j] >= threshold:\n",
    "                indices_to_remove.append(real_indices[i])\n",
    "\n",
    "    return indices_to_remove\n",
    "\n",
    "def remove_boxes(segments, boxes_to_remove):\n",
    "    n = segments[0]['scores'].shape[0]\n",
    "    new_scores = torch.tensor([segments[0]['scores'].tolist()[i] for i in range(n) if i not in boxes_to_remove], device=device)\n",
    "    new_labels = [segments[0]['labels'][i] for i in range(n) if i not in boxes_to_remove]\n",
    "    new_boxes = torch.tensor([segments[0]['boxes'].tolist()[i] for i in range(n) if i not in boxes_to_remove], device=device)\n",
    "    \n",
    "    return [{'scores': new_scores, 'labels': new_labels, 'boxes': new_boxes}]\n",
    "\n",
    "def get_overlapping_boxes(segments):  #overlapping w/ same labels\n",
    "    unique_labels = list(set(segments[0]['labels']))\n",
    "    boxes_to_remove = []\n",
    "\n",
    "    for l in unique_labels:\n",
    "        indices = [i for i in range(len(segments[0]['labels'])) if segments[0]['labels'][i] == l]\n",
    "        boxes = np.array([segments[0]['boxes'][i].cpu() for i in indices])\n",
    "        intersection_areas = compute_overlapping_area(boxes)\n",
    "        boxes_to_remove.extend(get_overlapping_index(intersection_areas, l, segments))\n",
    "\n",
    "    boxes_to_remove = list(set(boxes_to_remove))\n",
    "    return boxes_to_remove\n",
    "\n",
    "def get_similar_boxes(segments, threshold=.75): #similar w/ different (or same) labels\n",
    "    boxes_to_remove = []\n",
    "\n",
    "    intersection_areas = compute_overlapping_area(segments[0]['boxes'].cpu())\n",
    "\n",
    "    for i in range(intersection_areas.shape[0]):\n",
    "        for j in range(i):\n",
    "            if intersection_areas[i,j] >= threshold and intersection_areas[j,i] >= threshold:\n",
    "                idx = i if segments[0]['scores'][i] < segments[0]['scores'][j] else j\n",
    "                boxes_to_remove.append(idx)\n",
    "\n",
    "    boxes_to_remove = list(set(boxes_to_remove))\n",
    "    return boxes_to_remove\n",
    "\n",
    "def merge_labels(model, tokenizer, segments, display_similarity=False, threshold=.85):\n",
    "    real_labels = list(set(segments[0]['labels']))\n",
    "    inputs = tokenizer(real_labels, max_length=8192, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state[:, 0]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    scores = (embeddings @ embeddings.T)\n",
    "\n",
    "    similar_pairs = {}\n",
    "    for i in range(scores.shape[0]):\n",
    "        for j in range(i):\n",
    "            if display_similarity:\n",
    "                print(f\"similarity({real_labels[i], real_labels[j]})={scores[i,j]}\")\n",
    "            if scores[i,j] >= threshold:\n",
    "                label_to_be_replaced = real_labels[i] if len(real_labels[i]) >= len(real_labels[j]) else real_labels[j]\n",
    "                label_to_replace_with = real_labels[i] if label_to_be_replaced == real_labels[j] else real_labels[j]\n",
    "                similar_pairs[label_to_be_replaced] = label_to_replace_with\n",
    "\n",
    "    all_merged = False\n",
    "    while not all_merged:\n",
    "        all_merged = True\n",
    "        for k in range(len(segments[0]['labels'])):\n",
    "            if segments[0]['labels'][k] in similar_pairs.keys():\n",
    "                #print(f\"Merging label '{segments[0]['labels'][k]}' into '{similar_pairs[segments[0]['labels'][k]]}'...\")\n",
    "                segments[0]['labels'][k] = similar_pairs[segments[0]['labels'][k]]\n",
    "                all_merged = False\n",
    "\n",
    "    return segments\n",
    "    \n",
    "def controled_generation(model, processor, image, n_objects=10, **kwargs):\n",
    "    n_parallel = len(image)\n",
    "    prompt = f\"\"\"[INST] <image>\\nAnalyze the scene and infer what it is representing. Given the scene, list 5 to 10 objects or entities most likely to be part of the scene and most important to spot (use ONE SIMPLE WORD ONLY to describe an object or entity - this will be reprenting a category in large meaning). Ignore objects that are small or irrelevant to a blind person. Answer by filling out the following JSON format. Your answer must be parse-able with python's ast.literal_eval() - DO NOT ADD ANYTHING ELSE:\n",
    "    {{\n",
    "        \"title\": \"short_scene_title\",\n",
    "        \"objects\": {[f\"object_{i}\" for i in range(n_objects)]}\n",
    "    }}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    starter = \"\"\"   {\n",
    "        \"title\": \\\"\"\"\"\n",
    "\n",
    "    inputs = processor([prompt + starter] * n_parallel, image, return_tensors='pt').to(device, torch.float16)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, tokenizer=processor.tokenizer, stop_strings='\\\",', **kwargs)\n",
    "\n",
    "    intermediary = \"\"\"\n",
    "        \\\"objects\\\": ['\"\"\"\n",
    "\n",
    "    outputs = processor.tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    outputs = [re.sub(r'<s>|</s>|<pad>', '', o) + \"\\n    \\\"objects\\\": ['\" for o in outputs]\n",
    "\n",
    "    inputs['input_ids'], inputs['attention_mask'] = processor.tokenizer(outputs, return_tensors='pt', padding=True, add_special_tokens=False).to(device).values()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, tokenizer=processor.tokenizer, stop_strings=[\"]\", \"dummy string to circumvent bug in stop_strings\"], **kwargs)\n",
    "\n",
    "    ending = \"\"\"\n",
    "        }\"\"\"\n",
    "\n",
    "    outputs = processor.tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    outputs = [re.sub(r'<s>|</s>|<pad>', '', o)[len(prompt):] + \"\\n    }\" for o in outputs]\n",
    "    \n",
    "    return outputs\n",
    "    \n",
    "def post_processing(model_embeddings, tokenizer_embeddings, segments):\n",
    "    ## merging labels that are close in semantics\n",
    "    segments_merged_labels = merge_labels(model_embeddings, tokenizer_embeddings, segments, display_similarity=False)\n",
    "\n",
    "    ## removing boxes that are overlapping a lot with another same-label box\n",
    "    boxes_to_remove = get_overlapping_boxes(segments_merged_labels)\n",
    "    segments_sparser_boxes = remove_boxes(segments_merged_labels, boxes_to_remove)\n",
    "\n",
    "    ## removing boxes that are almost the same as an other but with a different label (keeping the one with highest score)\n",
    "    boxes_to_remove = get_similar_boxes(segments_sparser_boxes)\n",
    "    segments_cleaned = remove_boxes(segments_sparser_boxes, boxes_to_remove)\n",
    "\n",
    "    return segments_cleaned\n",
    "\n",
    "def run_image_description_phi(segments, model, tokenizer, image):\n",
    "    system_prompt = \"You are an AI model designed to help visually impaired people. Your task is to provide a comprehensive description of the image, locating important objects to guide disabled people through the scene.\"\n",
    "    \n",
    "    width, height = image.size\n",
    "    segments[0]['boxes'][:, 0::2] /= width\n",
    "    segments[0]['boxes'][:, 1::2] /= height\n",
    "\n",
    "    boxes = [row for row in segments[0]['boxes'].tolist()]\n",
    "    boxes = [str([round(x,2) for x in row]) for row in boxes]\n",
    "    labels = segments[0]['labels']\n",
    "    box_prompt = '\\n'.join(sorted([a + ' ' + b for a,b in zip(labels, boxes)]))\n",
    "\n",
    "    title = segments[0]['title']\n",
    "    question_prompt = f\"Below is a description of a {title} scene, along with a list of objects present in the scene along with their coordinates following the format 'object [x_min, y_min, x_max, y_max]'. Provide a descriptive paragraph using a human-like description, do not mention coordinates. Only use the position information and infer from it, do not add any comment or guess. Remain factual and avoid unnecessary embellishments, keep it simple.\"\n",
    "\n",
    "    sample_prompt = f\"\"\"<|system|>\n",
    "{system_prompt}<|end|>\n",
    "<|user|>\n",
    "{question_prompt}\n",
    "{box_prompt}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(sample_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "    \n",
    "    outputs = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_itt, processor_itt, model_segmentation, processor_segmentation, model_embeddings, tokenizer_embeddings = load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Phi3-mini-VD Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "import random as rd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707ad29b489a4c128855ca483edbdceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "checkpoint = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,\n",
    "                                             device_map=device,\n",
    "                                             attn_implementation='flash_attention_2',\n",
    "                                             cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_name = \"../models/phi3-mini-VD/checkpoint-20000\"\n",
    "model_llm = PeftModel.from_pretrained(model_llm, adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/val2014/'\n",
    "file_list = os.listdir(data_dir)\n",
    "\n",
    "img_path = data_dir + rd.choice(file_list)\n",
    "#img_path = 'street.jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a descriptive paragraph of the crowded city street scene:\n",
      "\n",
      "You are standing on a busy city sidewalk surrounded by tall buildings that stretch up towards the sky. The sidewalk stretches out before you, lined with people walking in all directions. To your left, there are several bicycles parked along the curb, some with bicyclists sitting on them. Ahead of you, there's a tram moving slowly through the traffic. On the right-hand side, you'll find more pedestrians making their way through the crowd. There are also some strollers being pushed by parents or caregivers. You'll notice several signs posted along the sidewalk, some attached to buildings and others standing alone. Amidst the hustle and bustle, there are a few trees providing shade for those walking by.\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "list_labels, title = get_labels(model_itt, processor_itt, image, n_objects=7, n_parallel_inference=3)\n",
    "\n",
    "segments = run_segmentation(model_segmentation, processor_segmentation, image, list_labels)\n",
    "\n",
    "segments_postprocessed = post_processing(model_embeddings, tokenizer_embeddings, segments)\n",
    "\n",
    "output_path = save_image_with_boxes(image, segments_postprocessed,f'test_boxed.jpg')\n",
    "\n",
    "segments_postprocessed[0]['title'] = title\n",
    "outputs = run_image_description_phi(segments_postprocessed, model_llm, tokenizer_llm, image)\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_description",
   "language": "python",
   "name": "visual_description"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
