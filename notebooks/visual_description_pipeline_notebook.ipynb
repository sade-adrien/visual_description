{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/et/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer, AutoModelForCausalLM, AutoModelForPreTraining, AutoModelForZeroShotObjectDetection, AutoModel\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from PIL import Image, ImageDraw\n",
    "import itertools\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a0229f899f43c8aa7549f77e9b0eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llava = 'llava-hf/llava-v1.6-mistral-7b-hf'\n",
    "device = 'cuda:0'\n",
    "\n",
    "processor_llava = AutoProcessor.from_pretrained(llava)\n",
    "model_llava = AutoModelForPreTraining.from_pretrained(llava,\n",
    "                                            low_cpu_mem_usage = True,\n",
    "                                            torch_dtype = torch.float16,\n",
    "                                            device_map=device,\n",
    "                                            cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eff1dd071ab49dc81aaaae5c88e6bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/457 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5436ecb53e4569acf0670070498389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df44b53706fb43449169244e80c48cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a130d6519ef4ff8a225f2476597d7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b171f8f8844cb29d4264aefff910b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/visual_description/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Some weights of GroundingDinoForObjectDetection were not initialized from the model checkpoint at IDEA-Research/grounding-dino-base and are newly initialized: ['bbox_embed.1.layers.0.bias', 'bbox_embed.1.layers.0.weight', 'bbox_embed.1.layers.1.bias', 'bbox_embed.1.layers.1.weight', 'bbox_embed.1.layers.2.bias', 'bbox_embed.1.layers.2.weight', 'bbox_embed.2.layers.0.bias', 'bbox_embed.2.layers.0.weight', 'bbox_embed.2.layers.1.bias', 'bbox_embed.2.layers.1.weight', 'bbox_embed.2.layers.2.bias', 'bbox_embed.2.layers.2.weight', 'bbox_embed.3.layers.0.bias', 'bbox_embed.3.layers.0.weight', 'bbox_embed.3.layers.1.bias', 'bbox_embed.3.layers.1.weight', 'bbox_embed.3.layers.2.bias', 'bbox_embed.3.layers.2.weight', 'bbox_embed.4.layers.0.bias', 'bbox_embed.4.layers.0.weight', 'bbox_embed.4.layers.1.bias', 'bbox_embed.4.layers.1.weight', 'bbox_embed.4.layers.2.bias', 'bbox_embed.4.layers.2.weight', 'bbox_embed.5.layers.0.bias', 'bbox_embed.5.layers.0.weight', 'bbox_embed.5.layers.1.bias', 'bbox_embed.5.layers.1.weight', 'bbox_embed.5.layers.2.bias', 'bbox_embed.5.layers.2.weight', 'model.decoder.bbox_embed.1.layers.0.bias', 'model.decoder.bbox_embed.1.layers.0.weight', 'model.decoder.bbox_embed.1.layers.1.bias', 'model.decoder.bbox_embed.1.layers.1.weight', 'model.decoder.bbox_embed.1.layers.2.bias', 'model.decoder.bbox_embed.1.layers.2.weight', 'model.decoder.bbox_embed.2.layers.0.bias', 'model.decoder.bbox_embed.2.layers.0.weight', 'model.decoder.bbox_embed.2.layers.1.bias', 'model.decoder.bbox_embed.2.layers.1.weight', 'model.decoder.bbox_embed.2.layers.2.bias', 'model.decoder.bbox_embed.2.layers.2.weight', 'model.decoder.bbox_embed.3.layers.0.bias', 'model.decoder.bbox_embed.3.layers.0.weight', 'model.decoder.bbox_embed.3.layers.1.bias', 'model.decoder.bbox_embed.3.layers.1.weight', 'model.decoder.bbox_embed.3.layers.2.bias', 'model.decoder.bbox_embed.3.layers.2.weight', 'model.decoder.bbox_embed.4.layers.0.bias', 'model.decoder.bbox_embed.4.layers.0.weight', 'model.decoder.bbox_embed.4.layers.1.bias', 'model.decoder.bbox_embed.4.layers.1.weight', 'model.decoder.bbox_embed.4.layers.2.bias', 'model.decoder.bbox_embed.4.layers.2.weight', 'model.decoder.bbox_embed.5.layers.0.bias', 'model.decoder.bbox_embed.5.layers.0.weight', 'model.decoder.bbox_embed.5.layers.1.bias', 'model.decoder.bbox_embed.5.layers.1.weight', 'model.decoder.bbox_embed.5.layers.2.bias', 'model.decoder.bbox_embed.5.layers.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "gdino = \"IDEA-Research/grounding-dino-base\"\n",
    "\n",
    "processor_gdino = AutoProcessor.from_pretrained(gdino)\n",
    "model_gdino = AutoModelForZeroShotObjectDetection.from_pretrained(gdino,\n",
    "                                                                low_cpu_mem_usage = True,\n",
    "                                                                device_map=device,\n",
    "                                                                cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "#phi3 = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer_phi3 = AutoTokenizer.from_pretrained(phi3)\n",
    "model_phi3 = AutoModelForCausalLM.from_pretrained(phi3,\n",
    "                                            low_cpu_mem_usage = True,\n",
    "                                            torch_dtype = torch.float16,\n",
    "                                            device_map=device,\n",
    "                                            cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                            #attn_implementation='flash_attention_2',\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_with_boxes(image, results):\n",
    "    boxes, labels, scores = results[0]['boxes'], results[0]['labels'], results[0]['scores']\n",
    "\n",
    "    img = image.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        box = box.cpu().tolist()\n",
    "        draw.rectangle(box, outline='red', width=4)\n",
    "\n",
    "        text = f'{label} ({score:.2f})'\n",
    "        draw.text([box[0]+5, box[1]+5], text, fill='white', font_size=15)\n",
    "\n",
    "    img.save('./output_image.jpg')    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene: 'modern bedroom'\n",
      "1. bed\n",
      "2. desk\n",
      "3. chair\n",
      "4. lamp\n",
      "5. plant\n",
      "6. dresser\n",
      "7. wall art\n",
      "8. window\n",
      "9. carpet\n",
      "10. fan \n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"[INST] <image>\\nAnalyze the scene and infer what it is representing. Given the scene, list the 10 objects or entities most likely to be part of the scene and most important to spot (use one-word objects rather than detailed descriptions). Answer using the following format, do not add any comment:\n",
    "Scene: 'Title'\n",
    "1. 'Object_1'\n",
    "2. 'Object_2'\n",
    "3. 'Object_3'\n",
    "... [/INST]\n",
    "\"\"\"\n",
    "\n",
    "image_file = './bedroom.jpeg'\n",
    "\n",
    "image = Image.open(image_file).convert('RGB')\n",
    "inputs = processor_llava(prompt, image, return_tensors='pt').to(device, torch.float16)\n",
    "\n",
    "outputs = model_llava.generate(**inputs, max_new_tokens=300)\n",
    "output = processor_llava.decode(outputs[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True).lower()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controled_generation(model, processor, image, n_objects=10, **kwargs):\n",
    "    pre_shot = \"\"\"[/INST]\n",
    "    {\n",
    "        \"title\": \"family dinner\",\n",
    "        \"objects\": ['children', 'parents', 'dishware', 'table', 'glasses', 'flatware', 'tablecloth', 'lights', 'chairs', 'food']\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"[INST] <image>\\nAnalyze the scene and infer what it is representing. Given the scene, list the {n_objects} objects or entities most likely to be part of the scene and most important to spot (use one-word objects rather than detailed descriptions). Answer by filling out the following JSON format. Your answer must be parse-able with python's ast.literal_eval() - DO NOT ADD ANYTHING ELSE:\n",
    "    {{\n",
    "        \"title\": \"scene_title\",\n",
    "        \"objects\": {[f\"object_{i}\" for i in range(n_objects)]}\n",
    "    }}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    starter = \"\"\"   {\n",
    "        \"title\": \\\"\"\"\"\n",
    "\n",
    "    inputs = processor(pre_shot + prompt + starter, image, return_tensors='pt').to(device, torch.float16)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10, tokenizer=processor.tokenizer, stop_strings='\\\",', **kwargs)\n",
    "\n",
    "    intermediary = \"\"\"\n",
    "        \\\"objects\\\": ['\"\"\"\n",
    "\n",
    "    input_ids, attention_mask = processor_llava.tokenizer(intermediary, return_tensors='pt').to(device).values() \n",
    "    input_ids, attention_mask = input_ids[:, 2:], attention_mask[:, 2:]\n",
    "\n",
    "    inputs['input_ids'] = torch.cat((outputs, input_ids), dim=-1)\n",
    "    inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], device=device)\n",
    "\n",
    "    for _ in range(n_objects):\n",
    "        outputs = model.generate(**inputs, max_new_tokens=5, tokenizer=processor.tokenizer, stop_strings=[\"'\", \"dummy string to circumvent bug in stop_strings\"], suppress_tokens=[2, 4709, 1421], **kwargs)\n",
    "\n",
    "        intermediary = \"\"\", '\"\"\"\n",
    "\n",
    "        input_ids, attention_mask = processor.tokenizer(intermediary, return_tensors='pt').to(device).values() \n",
    "        input_ids, attention_mask = input_ids[:, 2:], attention_mask[:, 2:]\n",
    "\n",
    "        inputs['input_ids'] = torch.cat((outputs, input_ids), dim=-1)\n",
    "        inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], device=device)\n",
    "    \n",
    "    ending = \"\"\"]\n",
    "        }\"\"\"\n",
    "\n",
    "    input_ids, _ = processor.tokenizer(ending, return_tensors='pt').to(device).values() \n",
    "    input_ids = input_ids[:, 1:]\n",
    "\n",
    "    outputs = torch.cat((outputs, input_ids), dim=-1)\n",
    "\n",
    "    return outputs[0, len(processor.tokenizer.encode(pre_shot + prompt)):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controled_generation2(model, processor, image, n_objects=10, **kwargs):\n",
    "    pre_shot = \"\"\"[/INST]\n",
    "    {\n",
    "        \"title\": \"family dinner\",\n",
    "        \"objects\": ['children', 'parents', 'dishware', 'table', 'glasses', 'flatware', 'tablecloth', 'lights', 'chairs', 'food']\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"[INST] <image>\\nAnalyze the scene and infer what it is representing. Given the scene, list the {n_objects} objects or entities most likely to be part of the scene and most important to spot (use one-word objects rather than detailed descriptions). Answer by filling out the following JSON format. Your answer must be parse-able with python's ast.literal_eval() - DO NOT ADD ANYTHING ELSE:\n",
    "    {{\n",
    "        \"title\": \"scene_title\",\n",
    "        \"objects\": {[f\"object_{i}\" for i in range(n_objects)]}\n",
    "    }}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    starter = \"\"\"   {\n",
    "        \"title\": \\\"\"\"\"\n",
    "\n",
    "    inputs = processor(pre_shot + prompt + starter, image, return_tensors='pt').to(device, torch.float16)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10, tokenizer=processor.tokenizer, stop_strings='\\\",', **kwargs)\n",
    "\n",
    "    intermediary = \"\"\"\n",
    "        \\\"objects\\\": ['\"\"\"\n",
    "\n",
    "    input_ids, attention_mask = processor.tokenizer(intermediary, return_tensors='pt').to(device).values() \n",
    "    input_ids, attention_mask = input_ids[:, 2:], attention_mask[:, 2:]\n",
    "\n",
    "    inputs['input_ids'] = torch.cat((outputs, input_ids), dim=-1)\n",
    "    inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], device=device)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=300, tokenizer=processor.tokenizer, stop_strings=[\"]\", \"dummy string to circumvent bug in stop_strings\"], **kwargs)\n",
    "\n",
    "    ending = \"\"\"\n",
    "        }\"\"\"\n",
    "\n",
    "    input_ids, _ = processor.tokenizer(ending, return_tensors='pt').to(device).values() \n",
    "    input_ids = input_ids[:, 1:]\n",
    "\n",
    "    outputs = torch.cat((outputs, input_ids), dim=-1)\n",
    "\n",
    "    return outputs[0, len(processor.tokenizer.encode(pre_shot + prompt)):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3828277587890625e-05 1.9133515357971191\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n=15\n",
    "\n",
    "x = time.time()\n",
    "#a = processor_llava.decode(controled_generation(model_llava, processor_llava, image, n_objects=n))\n",
    "y = time.time()\n",
    "b = processor_llava.decode(controled_generation2(model_llava, processor_llava, image, n_objects=n))\n",
    "z = time.time()\n",
    "\n",
    "print(y-x, z-y)\n",
    "#print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bed. chair. desk. lamp. pillows. dresser. plant. window. blinds. fan. rug. table. bookshelf. artwork.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast \n",
    "labels = ast.literal_eval(b)\n",
    "labels = '. '.join(labels['objects']).lower() +'.'\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene='bar with patrons'\n",
      "labels='bottles. shelves. bar stools. people. tables. chairs. lights. wine glasses. cocktails. menu.'\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"scene: '(?P<scene>[^']+)'\\n\"\n",
    "match = re.search(pattern, output)\n",
    "\n",
    "scene = match.group('scene')\n",
    "labels = '. '.join([re.sub(r'\\d+\\.', '', s).strip(' ') for s in output.split('\\n')[1:]+['']]).replace(\"'\",\"\").lower().strip()\n",
    "\n",
    "print(f\"{scene=}\\n{labels=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor_gdino(images=image, text=labels, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_gdino(**inputs)\n",
    "\n",
    "results = processor_gdino.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=0.2,\n",
    "        text_threshold=0.2,\n",
    "        target_sizes=[image.size[::-1]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scores': tensor([0.9314, 0.8986, 0.8057, 0.6813, 0.6343, 0.7857, 0.5726, 0.5148, 0.5023,\n",
       "          0.4878, 0.4638, 0.5440, 0.4545, 0.5005, 0.4537, 0.4389, 0.4258, 0.4181,\n",
       "          0.4038, 0.3561, 0.3757, 0.3549, 0.4028, 0.2109, 0.3641, 0.3426, 0.2013,\n",
       "          0.2743, 0.3211, 0.3881, 0.3838, 0.2425, 0.2386, 0.2066, 0.2308, 0.3488,\n",
       "          0.3161, 0.3555, 0.2428, 0.2380, 0.2301, 0.3048, 0.2945, 0.2610],\n",
       "         device='cuda:0'),\n",
       "  'labels': ['bed',\n",
       "   'fan',\n",
       "   'chair',\n",
       "   'window blinds',\n",
       "   'window blinds',\n",
       "   'window blinds',\n",
       "   'table',\n",
       "   'chair',\n",
       "   'window blinds',\n",
       "   'plant',\n",
       "   'window blinds',\n",
       "   'lamp',\n",
       "   'artwork',\n",
       "   'table',\n",
       "   'artwork',\n",
       "   'desk',\n",
       "   'lamp',\n",
       "   'dresser table',\n",
       "   'dresser',\n",
       "   'desk',\n",
       "   'pillows',\n",
       "   'table',\n",
       "   'dresser table',\n",
       "   'pillows',\n",
       "   'table',\n",
       "   'desk',\n",
       "   'artwork',\n",
       "   'pillows',\n",
       "   'pillows',\n",
       "   'pillows',\n",
       "   'desk',\n",
       "   'pillows',\n",
       "   'desk',\n",
       "   'pillows',\n",
       "   'artwork',\n",
       "   'dresser bookshelf',\n",
       "   'bookshelf',\n",
       "   'pillows',\n",
       "   'desk',\n",
       "   'dresser',\n",
       "   'window blinds',\n",
       "   'rug',\n",
       "   'plant',\n",
       "   'plant'],\n",
       "  'boxes': tensor([[4.6532e+02, 4.1113e+02, 9.1518e+02, 7.3994e+02],\n",
       "          [4.7433e+02, 1.1143e+00, 7.9219e+02, 1.5883e+02],\n",
       "          [5.8945e+01, 4.7972e+02, 3.4093e+02, 7.2094e+02],\n",
       "          [1.1230e+03, 2.1249e+02, 1.2394e+03, 4.2958e+02],\n",
       "          [8.9403e+02, 2.6332e+02, 9.9956e+02, 4.2223e+02],\n",
       "          [1.7373e+02, 1.6557e+02, 3.0705e+02, 4.3599e+02],\n",
       "          [7.0750e-01, 6.0047e+02, 1.2343e+02, 8.2426e+02],\n",
       "          [9.2981e+02, 4.3517e+02, 1.0094e+03, 5.5418e+02],\n",
       "          [7.6602e+02, 3.0409e+02, 7.9893e+02, 3.9435e+02],\n",
       "          [6.8904e+02, 3.9637e+02, 7.1858e+02, 4.4822e+02],\n",
       "          [6.7096e+02, 3.0281e+02, 7.0329e+02, 3.9648e+02],\n",
       "          [9.7493e+02, 3.4835e+02, 1.0854e+03, 4.4995e+02],\n",
       "          [5.7351e+02, 3.3837e+02, 6.2766e+02, 4.1612e+02],\n",
       "          [3.7139e+02, 4.8380e+02, 4.7620e+02, 6.0216e+02],\n",
       "          [4.9982e+02, 3.3150e+02, 5.6300e+02, 4.1741e+02],\n",
       "          [1.0961e+03, 4.9855e+02, 1.2395e+03, 8.2460e+02],\n",
       "          [3.0456e+02, 3.7096e+02, 4.5074e+02, 5.9895e+02],\n",
       "          [1.0961e+03, 4.9807e+02, 1.2395e+03, 8.2464e+02],\n",
       "          [8.5430e+02, 4.3595e+02, 9.3931e+02, 5.1924e+02],\n",
       "          [7.2117e-01, 6.0108e+02, 1.2324e+02, 8.2445e+02],\n",
       "          [5.6249e+02, 4.4667e+02, 6.5754e+02, 4.9661e+02],\n",
       "          [1.0039e+03, 4.4154e+02, 1.1095e+03, 5.6336e+02],\n",
       "          [8.5462e+02, 4.3504e+02, 1.1099e+03, 5.6295e+02],\n",
       "          [5.6254e+02, 4.3603e+02, 7.0636e+02, 4.9677e+02],\n",
       "          [7.0238e+02, 4.5984e+02, 7.5182e+02, 4.8263e+02],\n",
       "          [3.7157e+02, 4.8412e+02, 4.7610e+02, 6.0201e+02],\n",
       "          [4.9862e+02, 3.3130e+02, 6.2897e+02, 4.1841e+02],\n",
       "          [6.4956e+02, 4.3642e+02, 7.0593e+02, 4.8897e+02],\n",
       "          [4.8756e+02, 4.3585e+02, 7.0719e+02, 4.9862e+02],\n",
       "          [4.8913e+02, 4.4125e+02, 6.0211e+02, 4.9858e+02],\n",
       "          [8.5406e+02, 4.3544e+02, 1.1100e+03, 5.6298e+02],\n",
       "          [6.0501e+02, 4.3723e+02, 6.5914e+02, 4.5513e+02],\n",
       "          [1.0035e+03, 4.4193e+02, 1.1092e+03, 5.6321e+02],\n",
       "          [5.1183e+02, 4.4398e+02, 6.0118e+02, 4.9683e+02],\n",
       "          [9.4744e+02, 4.6683e+02, 9.9463e+02, 4.9772e+02],\n",
       "          [1.0956e+03, 4.9872e+02, 1.2396e+03, 8.2459e+02],\n",
       "          [8.5442e+02, 4.3660e+02, 9.3860e+02, 5.1857e+02],\n",
       "          [6.0549e+02, 4.3642e+02, 7.0580e+02, 4.8934e+02],\n",
       "          [7.0260e+02, 4.5994e+02, 7.5163e+02, 4.8230e+02],\n",
       "          [8.5325e+02, 4.3386e+02, 1.1102e+03, 5.6186e+02],\n",
       "          [6.7222e+02, 3.0587e+02, 6.9957e+02, 3.9416e+02],\n",
       "          [2.8018e+00, 5.4350e+02, 1.0066e+03, 8.2458e+02],\n",
       "          [8.9325e+02, 4.2181e+02, 9.1329e+02, 4.4004e+02],\n",
       "          [6.8822e+02, 3.9584e+02, 7.1893e+02, 4.6468e+02]], device='cuda:0')}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_image_with_boxes(image, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[100.0000,  82.2315,  46.2475],\n",
      "        [ 82.2315, 100.0000,  43.3766],\n",
      "        [ 46.2475,  43.3766, 100.0000]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real_labels = list(set(results[0]['labels']))\n",
    "device = 'cuda:0'\n",
    "real_labels = ['people bartender', 'bartender', 'people']\n",
    "\n",
    "gte = 'Alibaba-NLP/gte-base-en-v1.5'\n",
    "tokenizer_gte = AutoTokenizer.from_pretrained(gte)\n",
    "model_gte = AutoModel.from_pretrained(gte,\n",
    "                                    low_cpu_mem_usage = True,\n",
    "                                    device_map=device,\n",
    "                                    trust_remote_code=True,\n",
    "                                    cache_dir='/mnt/esperanto/et/huggingface/hub')\n",
    "\n",
    "inputs = tokenizer_gte(real_labels, max_length=8192, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "outputs = model_gte(**inputs)\n",
    "embeddings = outputs.last_hidden_state[:, 0]\n",
    "embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "scores = (embeddings @ embeddings.T) * 100\n",
    "print(scores)\n",
    "\n",
    "##threshold = 75/80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlapping_area(boxes):\n",
    "    areas = np.zeros((boxes.shape[0], boxes.shape[0]))\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "        for j in range(boxes.shape[0]):\n",
    "            x0 = np.maximum(boxes[i, 0], boxes[j, 0])\n",
    "            y0 = np.maximum(boxes[i, 1], boxes[j, 1])\n",
    "            x1 = np.minimum(boxes[i, 2], boxes[j, 2])\n",
    "            y1 = np.minimum(boxes[i, 3], boxes[j, 3])\n",
    "\n",
    "            width = np.maximum(0, x1 - x0)\n",
    "            height = np.maximum(0, y1 - y0)\n",
    "\n",
    "            base_box_area = np.maximum((boxes[i, 2] - boxes[i, 0]) * (boxes[i, 3] - boxes[i, 1]), 1e-5)\n",
    "            intersection_area = np.minimum(width * height / base_box_area, 1)\n",
    "\n",
    "            areas[i,j] = intersection_area\n",
    "    \n",
    "    return areas\n",
    "\n",
    "\n",
    "def get_index_to_remove(intersection_areas, current_label, labels, threshold=.8):\n",
    "    indices = []\n",
    "\n",
    "    for i in range(intersection_areas.shape[0]):\n",
    "        for j in range(intersection_areas.shape[1]):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if intersection_areas[i,j] >= threshold:\n",
    "                real_indices = [k for k, val in enumerate(labels) if val==current_label]\n",
    "                indices.append(real_indices[i])\n",
    "\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def remove_boxes(segments, boxes_to_remove):\n",
    "    n = segments[0]['scores'].shape[0]\n",
    "    new_scores = torch.tensor([segments[0]['scores'].tolist()[i] for i in range(n) if i not in boxes_to_remove], device=device)\n",
    "    new_labels = [segments[0]['labels'][i] for i in range(n) if i not in boxes_to_remove]\n",
    "    new_boxes = torch.tensor([segments[0]['boxes'].tolist()[i] for i in range(n) if i not in boxes_to_remove], device=device)\n",
    "    \n",
    "    return [{'scores': new_scores, 'labels': new_labels, 'boxes': new_boxes}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = list(set(results[0]['labels']))\n",
    "boxes_to_remove = []\n",
    "\n",
    "for l in unique_labels:\n",
    "    indices = [i for i in range(len(results[0]['labels'])) if results[0]['labels'][i] == l]\n",
    "    boxes = np.array([results[0]['boxes'][i].cpu() for i in indices])\n",
    "    intersection_areas = compute_overlapping_area(boxes)\n",
    "    boxes_to_remove.extend(get_index_to_remove(intersection_areas, l, results[0]['labels']))\n",
    "\n",
    "boxes_to_remove = list(set(boxes_to_remove))\n",
    "new_segments = remove_boxes(results, boxes_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scores': tensor([0.2911, 0.3231, 0.3487, 0.3450, 0.3104, 0.2165, 0.2351, 0.2389, 0.2179,\n",
       "          0.2755, 0.2542, 0.2079, 0.2152, 0.2491, 0.2798, 0.3607, 0.2582, 0.2231,\n",
       "          0.2598, 0.3126, 0.2005, 0.2661, 0.2040, 0.2738, 0.2059, 0.2092, 0.2154,\n",
       "          0.2315, 0.2679, 0.2686, 0.2640, 0.2384, 0.2785, 0.2291],\n",
       "         device='cuda:0'),\n",
       "  'labels': ['lights',\n",
       "   'bottles',\n",
       "   'bar stools chairs',\n",
       "   'people',\n",
       "   'bottles',\n",
       "   'bottles',\n",
       "   'wine glasses',\n",
       "   'lights',\n",
       "   'lights',\n",
       "   'bottles',\n",
       "   'bottles',\n",
       "   'tables',\n",
       "   'bottles',\n",
       "   'bottles',\n",
       "   'bottles',\n",
       "   'bottles',\n",
       "   'chairs',\n",
       "   'lights',\n",
       "   'bottles',\n",
       "   'bottles',\n",
       "   'tables',\n",
       "   'wine glasses',\n",
       "   'wine',\n",
       "   'bottles',\n",
       "   'wine',\n",
       "   'lights',\n",
       "   'chairs',\n",
       "   'shelves',\n",
       "   'bottles',\n",
       "   'wine glasses',\n",
       "   'bottles',\n",
       "   'bottles',\n",
       "   'bottles',\n",
       "   'bottles'],\n",
       "  'boxes': tensor([[ 4.2347e+01,  6.5028e+01,  1.5768e+02,  2.5501e+02],\n",
       "          [ 1.3113e+03,  6.5598e+02,  1.3368e+03,  7.4525e+02],\n",
       "          [ 2.9325e+02,  7.3910e+02,  4.1708e+02,  8.1684e+02],\n",
       "          [ 1.6088e-01,  4.0927e+02,  1.6001e+03,  8.9840e+02],\n",
       "          [ 1.3450e+03,  1.9721e+02,  1.5994e+03,  2.6166e+02],\n",
       "          [ 1.1801e+03,  2.6173e+02,  1.1972e+03,  3.1114e+02],\n",
       "          [ 1.2484e+03,  6.5621e+02,  1.3580e+03,  7.5835e+02],\n",
       "          [ 8.0501e+02,  1.6414e+02,  1.6001e+03,  3.1857e+02],\n",
       "          [ 8.8560e+02,  1.6844e+00,  9.7607e+02,  2.7954e+02],\n",
       "          [ 1.0401e+03,  3.3953e+02,  1.1574e+03,  3.9940e+02],\n",
       "          [ 1.3347e+03,  6.5815e+02,  1.3585e+03,  7.2586e+02],\n",
       "          [ 8.3000e+02,  7.7142e+02,  1.0544e+03,  8.9521e+02],\n",
       "          [ 1.0863e+03,  2.6428e+02,  1.1043e+03,  3.0624e+02],\n",
       "          [ 1.2823e+02,  1.0068e+02,  8.1389e+02,  2.1046e+02],\n",
       "          [ 6.8485e+01,  4.8727e+02,  2.2313e+02,  5.4737e+02],\n",
       "          [ 1.0380e+03,  4.1219e+02,  1.5994e+03,  4.7129e+02],\n",
       "          [ 1.1820e+03,  7.6586e+02,  1.2494e+03,  8.9843e+02],\n",
       "          [ 1.1999e+03,  4.4920e+01,  1.2694e+03,  3.0543e+02],\n",
       "          [ 1.1714e+03,  3.4224e+02,  1.4587e+03,  4.0319e+02],\n",
       "          [ 1.4683e+03,  3.4998e+02,  1.5993e+03,  4.0580e+02],\n",
       "          [ 3.0462e+02,  8.1378e+02,  5.3861e+02,  8.9786e+02],\n",
       "          [ 1.2528e+03,  4.9749e+02,  1.3266e+03,  5.6849e+02],\n",
       "          [ 1.1874e+03,  5.4812e+02,  1.2713e+03,  5.8303e+02],\n",
       "          [ 1.3356e+03,  2.6818e+02,  1.5993e+03,  3.2571e+02],\n",
       "          [ 1.2665e+03,  6.8767e+02,  1.2946e+03,  7.5772e+02],\n",
       "          [ 3.8262e+02, -1.3555e-01,  8.1544e+02,  8.7059e+01],\n",
       "          [ 4.5613e+02,  8.3157e+02,  5.7115e+02,  8.9885e+02],\n",
       "          [ 3.7227e+01,  4.8576e+00,  1.6034e+03,  5.7430e+02],\n",
       "          [ 5.9948e+01,  3.1096e+02,  8.2361e+02,  3.9165e+02],\n",
       "          [ 9.1751e+02,  7.3636e+02,  1.0231e+03,  8.0960e+02],\n",
       "          [ 6.5848e+01,  3.9990e+02,  8.1900e+02,  4.8120e+02],\n",
       "          [ 1.2530e+03,  2.6593e+02,  1.3227e+03,  3.1842e+02],\n",
       "          [ 1.2357e+03,  4.8326e+02,  1.5972e+03,  5.7108e+02],\n",
       "          [ 1.0513e+03,  1.7677e+02,  1.3147e+03,  2.4769e+02]], device='cuda:0')}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_image_with_boxes(image, new_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = image.size\n",
    "results[0]['boxes'][:, 0::2] /= width\n",
    "results[0]['boxes'][:, 1::2] /= height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed [0.38, 0.5, 0.74, 0.9]\n",
      "chair [0.05, 0.58, 0.27, 0.87]\n",
      "lamp [0.79, 0.42, 0.87, 0.54]\n",
      "plant [0.56, 0.48, 0.58, 0.54]\n",
      "chair [0.75, 0.53, 0.81, 0.67]\n",
      "lamp [0.25, 0.45, 0.36, 0.72]\n",
      "dresser [0.88, 0.6, 1.0, 1.0]\n",
      "dresser [0.69, 0.53, 0.76, 0.63]\n",
      "plant [0.72, 0.51, 0.74, 0.53]\n",
      "desk [0.0, 0.73, 0.1, 1.0]\n",
      "desk [0.69, 0.53, 0.89, 0.68]\n",
      "desk [0.81, 0.54, 0.89, 0.68]\n",
      "desk [0.88, 0.6, 1.0, 1.0]\n",
      "desk [0.3, 0.59, 0.38, 0.73]\n",
      "desk [0.57, 0.56, 0.61, 0.58]\n"
     ]
    }
   ],
   "source": [
    "boxes = [row for row in results[0]['boxes'].tolist()]\n",
    "boxes = [str([round(x,2) for x in row]) for row in boxes]\n",
    "#boxes = [re.sub(r'(\\.\\d\\d)\\d+', '', s) for s in boxes]\n",
    "labels = results[0]['labels']\n",
    "\n",
    "box_prompt = '\\n'.join([a + ' ' + b for a,b in zip(labels, boxes)])\n",
    "print(box_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to your cozy and inviting bedroom. As you enter, your feet touch the soft carpet beneath, inviting you to relax. To your left, you'll find a large, comfortable bed, upholstered in a soft, warm fabric. The bed is positioned towards the center of the room, with a plush headboard and footboard.\n",
      "\n",
      "Moving forward, there's a chair to your right, placed near the window. It's a perfect spot to sit and enjoy the morning sunlight or a quiet evening read.\n",
      "\n",
      "To the left of the bed, there's a bedside table with a lamp. The lamp casts a warm, gentle glow, illuminating the room and providing a soothing ambiance. Nearby, there's a small plant, adding a touch of nature and freshness to the room.\n",
      "\n",
      "Across from the bed, there's another chair, positioned near the dresser. This is a great place to put on your shoes or sit down to tie your laces.\n",
      "\n",
      "To the right of the bed, there's another bedside table with another lamp. This one also casts a warm, inviting glow, perfect for late-night reading or quiet reflection.\n",
      "\n",
      "In the corner of the room, there's a larger dresser, where you can find your clothes and personal items. There's another smaller dresser near the window, which could be used for storing smaller items or decorative pieces.\n",
      "\n",
      "Towards the center of the room, there's a desk, where you can work or study. There are two desks in total - one near the window and one towards the center of the room. Both desks have a lamp, providing ample light for your tasks.\n",
      "\n",
      "There are also two small plants in the room, one near the bedside table to the left of the bed and the other near the smaller dresser. They add a touch of natural beauty and help purify the air.\n",
      "\n",
      "I hope this description helps you navigate and enjoy your bedroom. Let me know if you have any other questions or need further clarification.\n"
     ]
    }
   ],
   "source": [
    "question = f\"Below is a description of a {scene} scene, along with a list of objects present in the scene along with their coordinates following the format 'object [x0, y0, x1, y1]'. Provide a descriptive paragraph to help the visually impaired people to understand the scene and find their way. Use a human-like description, do not mention coordinates.\\n{box_prompt}\"\n",
    "prompt = f\"<|user|>\\n{question} <|end|>\\n<|assistant|>\"\n",
    "\n",
    "inputs = tokenizer_phi3(prompt, return_tensors='pt').to(device)\n",
    "output = model_phi3.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer_phi3.decode(output[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this modern bedroom, you'll find a large bed, soft and inviting, situated towards the center of the room. The bed is surrounded by various pieces of furniture and decorative items. To your left as you face the bed, there's a circular fan, spinning gently to circulate the air.\n",
      "\n",
      "Moving towards the right, you'll come across a series of windows, letting in natural light. The first window is located near the headboard of the bed, followed by another one further to the right. A third window is situated near the foot of the bed, and a fourth one is located towards the far wall.\n",
      "\n",
      "As you move around the room, you'll notice a few chairs. One is positioned near the first window, while another one is located near the fourth window. There's also a desk with a lamp next to it, situated near the third window.\n",
      "\n",
      "Adding to the room's decor are several picture frames adorning the walls. Some are located near the windows, while others are placed on the desks or dressers. There's also a potted plant on the floor near the third window, adding a touch of nature to the room.\n",
      "\n",
      "Two dressers are present in the scene, one located towards the far wall and another one near the desk. Both dressers have multiple sections, providing ample storage space. The room is covered with a plush carpet, making every step comfortable underfoot.\n",
      "\n",
      "Lastly, there are two desks in the room, one located near the entrance and another one near the far wall. Both desks have lamps, providing adequate lighting for working or reading.\n"
     ]
    }
   ],
   "source": [
    "question = f\"Below is a description of a {scene} scene, along with a list of objects present in the scene along with their coordinates following the format 'object [x0, y0, x1, y1]'. Provide a descriptive paragraph to help the visually impaired people to understand the scene and find their way. Use a human-like description, do not mention coordinates.\\n{box_prompt}\"\n",
    "prompt = f\"[INST] {question} [/INST]\"\n",
    "\n",
    "inputs = tokenizer_phi3(prompt, return_tensors='pt').to(device)\n",
    "output = model_phi3.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer_phi3.decode(output[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_prompt = '[INST] <image>\\nMake a list of the major categories of objects and entities that are present in this scene. [/INST]'\n",
    "few_shots = f\"\"\"{inst_prompt} \n",
    "- Road\n",
    "- Sign\n",
    "- Bicycle\n",
    "- Stroller\n",
    "- Building\n",
    "- Bag\n",
    "- Tree\n",
    "- Lamppost\n",
    "- Pedestrian\n",
    "- Sky</s>\n",
    "{inst_prompt}\n",
    "- Desk\n",
    "- Chair\n",
    "- Laptop\n",
    "- Drink\n",
    "- Headphones\n",
    "- Lamp\n",
    "- Fresco\n",
    "- Person\n",
    "- Cable\n",
    "- Trash can\n",
    "- Bagpack</s>\n",
    "{inst_prompt}\n",
    "- Bottle\n",
    "- Bar counter\n",
    "- Stool\n",
    "- Glass\n",
    "- Table\n",
    "- Chair\n",
    "- Beer tap\n",
    "- Barman\n",
    "- Person\n",
    "- Cash register\n",
    "- Menu\n",
    "- Chalkboard\n",
    "- Candle\n",
    "- Light</s>\n",
    "\"\"\"\n",
    "image_files = ['./street.jpeg', './office.jpeg', './bar.jpeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bed. desk. chair. lamp. dresser. potted plant. picture frame. window. carpet. fan.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = re.sub(r'\\d+\\.', '', output).replace('\\n', '.').strip(\" \").lower() + '.'\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['furniture',\n",
       " 'lighting',\n",
       " 'decorative items',\n",
       " 'plants',\n",
       " 'electronics',\n",
       " 'storage',\n",
       " 'seating',\n",
       " 'textiles',\n",
       " 'windows',\n",
       " 'flooring']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = re.sub(r'\\d+\\.', '', output).lower().split('\\n')\n",
    "labels = [l.strip(' ') for l in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'scores': tensor([], device='cuda:0'), 'labels': [], 'boxes': tensor([], device='cuda:0', size=(0, 4))}]\n",
      "[{'scores': tensor([], device='cuda:0'), 'labels': [], 'boxes': tensor([], device='cuda:0', size=(0, 4))}]\n",
      "[{'scores': tensor([], device='cuda:0'), 'labels': [], 'boxes': tensor([], device='cuda:0', size=(0, 4))}]\n",
      "[{'scores': tensor([], device='cuda:0'), 'labels': [], 'boxes': tensor([], device='cuda:0', size=(0, 4))}]\n",
      "[{'scores': tensor([], device='cuda:0'), 'labels': [], 'boxes': tensor([], device='cuda:0', size=(0, 4))}]\n",
      "[{'scores': tensor([], device='cuda:0'), 'labels': [], 'boxes': tensor([], device='cuda:0', size=(0, 4))}]\n",
      "[{'scores': tensor([], device='cuda:0'), 'labels': [], 'boxes': tensor([], device='cuda:0', size=(0, 4))}]\n",
      "[{'scores': tensor([], device='cuda:0'), 'labels': [], 'boxes': tensor([], device='cuda:0', size=(0, 4))}]\n"
     ]
    }
   ],
   "source": [
    "results = None\n",
    "\n",
    "for label in labels:\n",
    "    inputs = processor_gdino(images=image, text=label, return_tensors=\"pt\").to(device)\n",
    "    #inputs = BatchEncoding({k:v.to(torch.float16) if k=='pixel_values' else v for (k,v) in inputs.items()}).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_gdino(**inputs)\n",
    "\n",
    "    temp = processor_gdino.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            box_threshold=0.3,\n",
    "            text_threshold=0.3,\n",
    "            target_sizes=[image.size[::-1]]\n",
    "        )\n",
    "\n",
    "    if results is None:\n",
    "        results = temp\n",
    "    else:\n",
    "        results[0]['scores'] = torch.cat((results[0]['scores'], temp[0]['scores']))\n",
    "        results[0]['labels'] += temp[0]['labels']\n",
    "        results[0]['boxes'] = torch.cat((results[0]['boxes'], temp[0]['boxes']))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scores': tensor([0.1237, 0.1419, 0.2412, 0.1309, 0.1020, 0.1242, 0.1121, 0.2139, 0.1290,\n",
       "          0.1676, 0.1065, 0.1046, 0.1099, 0.2857, 0.1574, 0.1315, 0.1313, 0.1164,\n",
       "          0.1034, 0.1029, 0.1198, 0.1557], device='cuda:0'),\n",
       "  'labels': ['furniture',\n",
       "   'lighting',\n",
       "   'items',\n",
       "   'items',\n",
       "   'items',\n",
       "   'plants',\n",
       "   'electronics',\n",
       "   'storage',\n",
       "   'storage',\n",
       "   'storage',\n",
       "   'storage',\n",
       "   'storage',\n",
       "   'storage',\n",
       "   'seating',\n",
       "   'seating',\n",
       "   'seating',\n",
       "   'seating',\n",
       "   'seating',\n",
       "   'seating',\n",
       "   'seating',\n",
       "   'seating',\n",
       "   'textiles'],\n",
       "  'boxes': tensor([[ 464.9793,  412.3817,  900.5969,  739.4567],\n",
       "          [ 464.4186,  412.8203,  901.9557,  739.9791],\n",
       "          [ 465.7617,  413.2639,  900.6996,  740.0132],\n",
       "          [  60.1792,  481.2223,  338.8812,  719.7956],\n",
       "          [1097.7428,  498.9970, 1239.2784,  824.3861],\n",
       "          [ 465.4456,  412.8286,  901.1208,  739.2968],\n",
       "          [ 465.7477,  412.5522,  900.6743,  739.1409],\n",
       "          [ 465.5687,  412.9430,  900.5609,  739.5320],\n",
       "          [  60.2045,  481.0681,  338.9561,  719.5139],\n",
       "          [ 855.4089,  437.9478,  938.5292,  516.6390],\n",
       "          [ 372.4442,  484.9351,  475.1617,  601.4553],\n",
       "          [ 702.7725,  461.9992,  750.7994,  481.5458],\n",
       "          [ 372.9476,  486.1598,  474.7586,  542.5761],\n",
       "          [ 465.9704,  413.0166,  901.3314,  739.3860],\n",
       "          [  60.3127,  481.2862,  338.8856,  719.5367],\n",
       "          [ 372.3749,  484.8330,  474.8143,  601.2860],\n",
       "          [ 855.2458,  437.8133,  938.6710,  516.7595],\n",
       "          [1097.8116,  498.9784, 1239.2654,  824.2606],\n",
       "          [ 303.9890,  372.6336,  450.0350,  597.6959],\n",
       "          [1005.4183,  444.3256, 1108.3671,  562.9259],\n",
       "          [ 931.9266,  436.1411, 1008.1243,  553.7343],\n",
       "          [ 465.3032,  412.7772,  900.6142,  739.3497]], device='cuda:0')}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_image_with_boxes(image, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont, Image\n",
    "\n",
    "image_file = './bedroom.jpeg'\n",
    "\n",
    "image = Image.open(image_file).convert('RGB')\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "box = [400, 400]\n",
    "\n",
    "text = \"Your text here\"\n",
    "position = (box[0]+5, box[1]+5)\n",
    "\n",
    "# Define the font and font size\n",
    "font = ImageFont.load_default()\n",
    "font = font.font_variant(size=15)\n",
    "\n",
    "# Get the size of the text\n",
    "text_bbox = draw.textbbox(position, text, font=font)\n",
    "text_width = text_bbox[2] - text_bbox[0] + 2\n",
    "text_height = text_bbox[3] - text_bbox[1] + 2\n",
    "\n",
    "# Define the background rectangle\n",
    "background_rectangle = [position[0] - 1, position[1]+5 - 1, position[0]+text_width - 1, position[1]+5+text_height - 1]\n",
    "\n",
    "# Draw the filled background rectangle\n",
    "draw.rectangle(background_rectangle, fill='grey')\n",
    "\n",
    "# Draw the text\n",
    "draw.text(position, text, fill='white', font=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, 826)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1240"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import streamlit as st\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoModel, AutoModelForCausalLM, LlavaNextForConditionalGeneration, AutoModelForZeroShotObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageColor, ImageFont\n",
    "import numpy as np\n",
    "import torch\n",
    "import ast\n",
    "import re\n",
    "\n",
    "device = 'cuda:0'\n",
    "image_to_text_model = 'llava-hf/llava-v1.6-mistral-7b-hf'\n",
    "segmentation_model = \"IDEA-Research/grounding-dino-base\"\n",
    "embeddings_model = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "\n",
    "#def fcts\n",
    "def load_models():\n",
    "    processor_itt = AutoProcessor.from_pretrained(image_to_text_model)\n",
    "    model_itt = LlavaNextForConditionalGeneration.from_pretrained(image_to_text_model,\n",
    "                                                low_cpu_mem_usage = True,\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                device_map=device,\n",
    "                                                cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                                )\n",
    "\n",
    "    processor_segmentation = AutoProcessor.from_pretrained(segmentation_model)\n",
    "    model_segmentation = AutoModelForZeroShotObjectDetection.from_pretrained(segmentation_model,\n",
    "                                                                    low_cpu_mem_usage = True,\n",
    "                                                                    device_map=device,\n",
    "                                                                    cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                                                    )\n",
    "\n",
    "    tokenizer_embeddings = AutoTokenizer.from_pretrained(embeddings_model)\n",
    "    model_embeddings = AutoModel.from_pretrained(embeddings_model,\n",
    "                                        low_cpu_mem_usage = True,\n",
    "                                        torch_dtype=torch.float16,\n",
    "                                        device_map=device,\n",
    "                                        trust_remote_code=True,\n",
    "                                        cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                        )\n",
    "                        \n",
    "    return model_itt, processor_itt, model_segmentation, processor_segmentation, model_embeddings, tokenizer_embeddings\n",
    "\n",
    "def get_labels_color(labels):\n",
    "    all_colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown', 'gray', 'cyan', 'magenta', 'lime', 'teal', 'navy', 'maroon', 'olive', 'indigo', 'violet', 'coral', 'salmon', 'gold', 'silver', 'turquoise', 'lavender', 'beige', 'tan', 'mint', 'plum', 'khaki', 'ivory', 'honeydew']\n",
    "    labels = list(set(labels))\n",
    "\n",
    "    labels_to_colors = [[label, color] for label, color in zip(labels, all_colors)]\n",
    "\n",
    "    return dict(labels_to_colors)\n",
    "\n",
    "def save_image_with_boxes(image, segments):\n",
    "    path = './output_image.jpg'\n",
    "\n",
    "    boxes, labels, scores = segments[0]['boxes'], segments[0]['labels'], segments[0]['scores']\n",
    "    \n",
    "    labels_to_colors = get_labels_color(labels)\n",
    "\n",
    "    img = image.copy().convert(\"RGBA\")\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "\n",
    "        box_width = max(1, int(0.0035 * max(img.size)))\n",
    "        text_size = max(9, int(0.01 * max(img.size)))\n",
    "\n",
    "\n",
    "        overlay = Image.new('RGBA', img.size, (0,0,0,0))\n",
    "        draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "        transparency = 200\n",
    "        box = box.cpu().tolist()\n",
    "        outline_color = ImageColor.getcolor(labels_to_colors[label], \"RGB\")\n",
    "        draw.rectangle(box, outline=(outline_color[0], outline_color[1], outline_color[2], transparency), width=box_width)\n",
    "\n",
    "        font = ImageFont.load_default()\n",
    "        font = font.font_variant(size=text_size)\n",
    "\n",
    "        position = (box[0] + box_width, box[1] + box_width)\n",
    "        text_bbox = draw.textbbox(position, label, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0] + 2\n",
    "        text_height = text_bbox[3] - text_bbox[1] + 2\n",
    "        background_rectangle = [position[0], position[1], position[0] + text_width, position[1] + text_height]\n",
    "        background_rectangle = [val + 2 if i%2==1 else val for i, val in enumerate(background_rectangle)] #add some margin to fit the text position\n",
    "\n",
    "        draw.rectangle(background_rectangle, fill='grey')\n",
    "        draw.text(position, label, fill='white', font=font)\n",
    "\n",
    "        img = Image.alpha_composite(img, overlay)\n",
    "\n",
    "    img = img.convert(\"RGB\")\n",
    "    img.save(path)\n",
    "    return path\n",
    "\n",
    "def get_labels(model, processor, image, n_objects=10, n_parallel_inference=3):\n",
    "    generation_success = False\n",
    "    while not generation_success:\n",
    "        outputs = controled_generation(model, processor, [image] * n_parallel_inference, n_objects=n_objects, do_sample=True, temperature=0.7)\n",
    "        try:\n",
    "            list_outputs = []\n",
    "            for output in outputs:\n",
    "                list_outputs.append(ast.literal_eval(output))\n",
    "            if len(list_outputs[-1]['objects']) <= n_objects*1.2 and len(list_outputs[-1]['objects']) >= n_objects*0.8:\n",
    "                generation_success = True\n",
    "            else:\n",
    "                print(f\"Failed to generate the right number of labels: {len(list_outputs[-1]['objects'])}\")\n",
    "        except:\n",
    "            print(f\"An error occured during json evaluation of generated output: {outputs}\")\n",
    "            continue\n",
    "    \n",
    "    list_labels = ['. '.join(list_outputs[i]['objects']).lower() +'.' for i in range(n_parallel_inference)]\n",
    "    #### check if a list of titles can be usefull\n",
    "    title = list_outputs[0]['title']\n",
    "\n",
    "    return list_labels, title\n",
    "    \n",
    "def run_segmentation(model, processor, image, list_labels):\n",
    "    n_parallel = len(list_labels)\n",
    "    inputs = processor(images=[image] * n_parallel, text=list_labels, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(outputs,\n",
    "                                                            inputs.input_ids,\n",
    "                                                            box_threshold=0.2,\n",
    "                                                            text_threshold=0.2,\n",
    "                                                            target_sizes=[image.size[::-1]] * n_parallel\n",
    "                                                            )\n",
    "\n",
    "    results = [{\n",
    "                'scores': torch.concatenate([results[i]['scores'] for i in range(n_parallel)]),\n",
    "                'labels': [l for sublist in [results[i]['labels'] for i in range(n_parallel)] for l in sublist],\n",
    "                'boxes': torch.concatenate([results[i]['boxes'] for i in range(n_parallel)])\n",
    "            }]\n",
    "\n",
    "    return results\n",
    "\n",
    "def compute_overlapping_area(boxes):\n",
    "    areas = np.zeros((boxes.shape[0], boxes.shape[0]))\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "        for j in range(boxes.shape[0]):\n",
    "            x0 = max(boxes[i, 0], boxes[j, 0])\n",
    "            y0 = max(boxes[i, 1], boxes[j, 1])\n",
    "            x1 = min(boxes[i, 2], boxes[j, 2])\n",
    "            y1 = min(boxes[i, 3], boxes[j, 3])\n",
    "\n",
    "            width = max(0, x1 - x0)\n",
    "            height = max(0, y1 - y0)\n",
    "\n",
    "            base_box_area = max((boxes[i, 2] - boxes[i, 0]) * (boxes[i, 3] - boxes[i, 1]), 1e-5)\n",
    "            intersection_area = min(width * height / base_box_area, 1)\n",
    "\n",
    "            areas[i,j] = intersection_area\n",
    "    \n",
    "    return areas\n",
    "\n",
    "def get_overlapping_index(intersection_areas, current_label, segments, threshold=.7):\n",
    "    indices_to_remove = []\n",
    "    real_indices = [k for k, val in enumerate(segments[0]['labels']) if val==current_label]\n",
    "\n",
    "    for i in range(intersection_areas.shape[0]):\n",
    "        for j in range(intersection_areas.shape[1]):\n",
    "            if i == j or real_indices[i] in indices_to_remove or real_indices[j] in indices_to_remove:\n",
    "                continue\n",
    "            if intersection_areas[i,j] >= threshold:\n",
    "                indices_to_remove.append(real_indices[i])\n",
    "\n",
    "    return indices_to_remove\n",
    "\n",
    "def remove_boxes(segments, boxes_to_remove):\n",
    "    n = segments[0]['scores'].shape[0]\n",
    "    new_scores = torch.tensor([segments[0]['scores'].tolist()[i] for i in range(n) if i not in boxes_to_remove], device=device)\n",
    "    new_labels = [segments[0]['labels'][i] for i in range(n) if i not in boxes_to_remove]\n",
    "    new_boxes = torch.tensor([segments[0]['boxes'].tolist()[i] for i in range(n) if i not in boxes_to_remove], device=device)\n",
    "    \n",
    "    return [{'scores': new_scores, 'labels': new_labels, 'boxes': new_boxes}]\n",
    "\n",
    "def get_overlapping_boxes(segments):  #overlapping w/ same labels\n",
    "    unique_labels = list(set(segments[0]['labels']))\n",
    "    boxes_to_remove = []\n",
    "\n",
    "    for l in unique_labels:\n",
    "        indices = [i for i in range(len(segments[0]['labels'])) if segments[0]['labels'][i] == l]\n",
    "        boxes = np.array([segments[0]['boxes'][i].cpu() for i in indices])\n",
    "        intersection_areas = compute_overlapping_area(boxes)\n",
    "        boxes_to_remove.extend(get_overlapping_index(intersection_areas, l, segments))\n",
    "\n",
    "    boxes_to_remove = list(set(boxes_to_remove))\n",
    "    return boxes_to_remove\n",
    "\n",
    "def get_similar_boxes(segments, threshold=.75): #similar w/ different (or same) labels\n",
    "    boxes_to_remove = []\n",
    "\n",
    "    intersection_areas = compute_overlapping_area(segments[0]['boxes'].cpu())\n",
    "\n",
    "    for i in range(intersection_areas.shape[0]):\n",
    "        for j in range(i):\n",
    "            if intersection_areas[i,j] >= threshold and intersection_areas[j,i] >= threshold:\n",
    "                idx = i if segments[0]['scores'][i] < segments[0]['scores'][j] else j\n",
    "                boxes_to_remove.append(idx)\n",
    "\n",
    "    boxes_to_remove = list(set(boxes_to_remove))\n",
    "    return boxes_to_remove\n",
    "\n",
    "def merge_labels(model, tokenizer, segments, display_similarity=False, threshold=.85):\n",
    "    real_labels = list(set(segments[0]['labels']))\n",
    "    inputs = tokenizer(real_labels, max_length=8192, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state[:, 0]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    scores = (embeddings @ embeddings.T)\n",
    "\n",
    "    similar_pairs = {}\n",
    "    for i in range(scores.shape[0]):\n",
    "        for j in range(i):\n",
    "            if display_similarity:\n",
    "                print(f\"similarity({real_labels[i], real_labels[j]})={scores[i,j]}\")\n",
    "            if scores[i,j] >= threshold:\n",
    "                label_to_be_replaced = real_labels[i] if len(real_labels[i]) >= len(real_labels[j]) else real_labels[j]\n",
    "                label_to_replace_with = real_labels[i] if label_to_be_replaced == real_labels[j] else real_labels[j]\n",
    "                similar_pairs[label_to_be_replaced] = label_to_replace_with\n",
    "\n",
    "    all_merged = False\n",
    "    while not all_merged:\n",
    "        all_merged = True\n",
    "        for k in range(len(segments[0]['labels'])):\n",
    "            if segments[0]['labels'][k] in similar_pairs.keys():\n",
    "                print(f\"Merging label '{segments[0]['labels'][k]}' into '{similar_pairs[segments[0]['labels'][k]]}'...\")\n",
    "                segments[0]['labels'][k] = similar_pairs[segments[0]['labels'][k]]\n",
    "                all_merged = False\n",
    "\n",
    "    return segments\n",
    "    \n",
    "def controled_generation(model, processor, image, n_objects=10, **kwargs):\n",
    "    n_parallel = len(image)\n",
    "    prompt = f\"\"\"[INST] <image>\\nAnalyze the scene and infer what it is representing. Given the scene, list the {n_objects} objects or entities most likely to be part of the scene and most important to spot (use ONE SIMPLE WORD ONLY to describe an object or entity - this will be reprenting a category in large meaning). Ignore objects that are small or irrelevant to a blind person. Answer by filling out the following JSON format. Your answer must be parse-able with python's ast.literal_eval() - DO NOT ADD ANYTHING ELSE:\n",
    "    {{\n",
    "        \"title\": \"short_scene_title\",\n",
    "        \"objects\": {[f\"object_{i}\" for i in range(n_objects)]}\n",
    "    }}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    starter = \"\"\"   {\n",
    "        \"title\": \\\"\"\"\"\n",
    "\n",
    "    inputs = processor([prompt + starter] * n_parallel, image, return_tensors='pt').to(device, torch.float16)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, tokenizer=processor.tokenizer, stop_strings='\\\",', **kwargs)\n",
    "\n",
    "    intermediary = \"\"\"\n",
    "        \\\"objects\\\": ['\"\"\"\n",
    "\n",
    "    outputs = processor.tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    outputs = [re.sub(r'<s>|</s>|<pad>', '', o) + \"\\n    \\\"objects\\\": ['\" for o in outputs]\n",
    "\n",
    "    inputs['input_ids'], inputs['attention_mask'] = processor.tokenizer(outputs, return_tensors='pt', padding=True, add_special_tokens=False).to(device).values()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, tokenizer=processor.tokenizer, stop_strings=[\"]\", \"dummy string to circumvent bug in stop_strings\"], **kwargs)\n",
    "\n",
    "    ending = \"\"\"\n",
    "        }\"\"\"\n",
    "\n",
    "    outputs = processor.tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    outputs = [re.sub(r'<s>|</s>|<pad>', '', o)[len(prompt):] + \"\\n    }\" for o in outputs]\n",
    "    \n",
    "    return outputs\n",
    "    \n",
    "def post_processing(model_embeddings, tokenizer_embeddings, segments):\n",
    "    ## merging labels that are close in semantics\n",
    "    segments_merged_labels = merge_labels(model_embeddings, tokenizer_embeddings, segments, display_similarity=False)\n",
    "\n",
    "    ## removing boxes that are overlapping a lot with another same-label box\n",
    "    boxes_to_remove = get_overlapping_boxes(segments_merged_labels)\n",
    "    segments_sparser_boxes = remove_boxes(segments_merged_labels, boxes_to_remove)\n",
    "\n",
    "    ## removing boxes that are almost the same as an other but with a different label (keeping the one with highest score)\n",
    "    boxes_to_remove = get_similar_boxes(segments_sparser_boxes)\n",
    "    segments_cleaned = remove_boxes(segments_sparser_boxes, boxes_to_remove)\n",
    "\n",
    "    return segments_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49113675830478e9a64186e5d429b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/visual_description/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Some weights of GroundingDinoForObjectDetection were not initialized from the model checkpoint at IDEA-Research/grounding-dino-base and are newly initialized: ['bbox_embed.1.layers.0.bias', 'bbox_embed.1.layers.0.weight', 'bbox_embed.1.layers.1.bias', 'bbox_embed.1.layers.1.weight', 'bbox_embed.1.layers.2.bias', 'bbox_embed.1.layers.2.weight', 'bbox_embed.2.layers.0.bias', 'bbox_embed.2.layers.0.weight', 'bbox_embed.2.layers.1.bias', 'bbox_embed.2.layers.1.weight', 'bbox_embed.2.layers.2.bias', 'bbox_embed.2.layers.2.weight', 'bbox_embed.3.layers.0.bias', 'bbox_embed.3.layers.0.weight', 'bbox_embed.3.layers.1.bias', 'bbox_embed.3.layers.1.weight', 'bbox_embed.3.layers.2.bias', 'bbox_embed.3.layers.2.weight', 'bbox_embed.4.layers.0.bias', 'bbox_embed.4.layers.0.weight', 'bbox_embed.4.layers.1.bias', 'bbox_embed.4.layers.1.weight', 'bbox_embed.4.layers.2.bias', 'bbox_embed.4.layers.2.weight', 'bbox_embed.5.layers.0.bias', 'bbox_embed.5.layers.0.weight', 'bbox_embed.5.layers.1.bias', 'bbox_embed.5.layers.1.weight', 'bbox_embed.5.layers.2.bias', 'bbox_embed.5.layers.2.weight', 'model.decoder.bbox_embed.1.layers.0.bias', 'model.decoder.bbox_embed.1.layers.0.weight', 'model.decoder.bbox_embed.1.layers.1.bias', 'model.decoder.bbox_embed.1.layers.1.weight', 'model.decoder.bbox_embed.1.layers.2.bias', 'model.decoder.bbox_embed.1.layers.2.weight', 'model.decoder.bbox_embed.2.layers.0.bias', 'model.decoder.bbox_embed.2.layers.0.weight', 'model.decoder.bbox_embed.2.layers.1.bias', 'model.decoder.bbox_embed.2.layers.1.weight', 'model.decoder.bbox_embed.2.layers.2.bias', 'model.decoder.bbox_embed.2.layers.2.weight', 'model.decoder.bbox_embed.3.layers.0.bias', 'model.decoder.bbox_embed.3.layers.0.weight', 'model.decoder.bbox_embed.3.layers.1.bias', 'model.decoder.bbox_embed.3.layers.1.weight', 'model.decoder.bbox_embed.3.layers.2.bias', 'model.decoder.bbox_embed.3.layers.2.weight', 'model.decoder.bbox_embed.4.layers.0.bias', 'model.decoder.bbox_embed.4.layers.0.weight', 'model.decoder.bbox_embed.4.layers.1.bias', 'model.decoder.bbox_embed.4.layers.1.weight', 'model.decoder.bbox_embed.4.layers.2.bias', 'model.decoder.bbox_embed.4.layers.2.weight', 'model.decoder.bbox_embed.5.layers.0.bias', 'model.decoder.bbox_embed.5.layers.0.weight', 'model.decoder.bbox_embed.5.layers.1.bias', 'model.decoder.bbox_embed.5.layers.1.weight', 'model.decoder.bbox_embed.5.layers.2.bias', 'model.decoder.bbox_embed.5.layers.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_itt, processor_itt, model_segmentation, processor_segmentation, model_embeddings, tokenizer_embeddings = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# mistral = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "gemma2 = \"google/gemma-2-27b-it\"\n",
    "\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(gemma2)\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(gemma2,\n",
    "                                            low_cpu_mem_usage = True,\n",
    "                                            torch_dtype = torch.bfloat16,\n",
    "                                            load_in_4bit=True,\n",
    "                                            device_map=device,\n",
    "                                            cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                            attn_implementation='flash_attention_2',\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging label 'stuffed animal' into 'teddy bear'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'drawers' into 'dresser'...\n",
      "Merging label 'drawers' into 'dresser'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'drawers' into 'dresser'...\n",
      "Merging label 'drawers' into 'dresser'...\n",
      "Merging label 'drawers' into 'dresser'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'drawers' into 'dresser'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n",
      "Merging label 'clothes' into 'dress'...\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('./image-description-sequences/data/image_data/ADE20K_2016_07_26/images/training/c/closet/ADE_train_00005724.jpg').convert('RGB')\n",
    "\n",
    "list_labels, title = get_labels(model_itt, processor_itt, image, n_objects=10, n_parallel_inference=3)\n",
    "\n",
    "segments = run_segmentation(model_segmentation, processor_segmentation, image, list_labels)\n",
    "\n",
    "segments_postprocessed = post_processing(model_embeddings, tokenizer_embeddings, segments)\n",
    "\n",
    "output_path = save_image_with_boxes(image, segments_postprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chairs [-0.0, 0.53, 0.24, 0.98]\n",
      "chairs [0.34, 0.43, 0.47, 0.58]\n",
      "chairs [0.4, 0.63, 0.72, 1.0]\n",
      "chairs [0.48, 0.42, 0.77, 0.83]\n",
      "chairs [0.89, 0.6, 1.0, 0.98]\n",
      "door [0.04, 0.02, 0.26, 0.75]\n",
      "door [0.3, 0.04, 0.85, 0.87]\n",
      "door [0.89, 0.02, 1.0, 0.75]\n",
      "plant [0.4, 0.29, 0.72, 0.6]\n",
      "pool [0.36, 0.4, 0.69, 0.51]\n",
      "table [0.16, 0.57, 0.97, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "segments = copy.deepcopy(segments_postprocessed)\n",
    "\n",
    "width, height = image.size\n",
    "segments[0]['boxes'][:, 0::2] /= width\n",
    "segments[0]['boxes'][:, 1::2] /= height\n",
    "\n",
    "boxes = [row for row in segments[0]['boxes'].tolist()]\n",
    "boxes = [str([round(x,2) for x in row]) for row in boxes]\n",
    "\n",
    "labels = segments[0]['labels']\n",
    "\n",
    "box_prompt = '\\n'.join(sorted([a + ' ' + b for a,b in zip(labels, boxes)]))\n",
    "print(box_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWrite a hello world program<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n",
    "]\n",
    "prompt = tokenizer_llm.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<bos><start_of_turn>user\n",
    "You are an AI model designed to help visually impaired people. Your task is to provide helpful detailed description of a scenery to guide people. \n",
    "Below is a description of a modern bedroom scene, along with a list of objects present in the scene along with their coordinates following the format 'object [x_min, y_min, x_max, y_max]'. Provide a descriptive paragraph using a human-like description, do not mention coordinates. Only use the position information and infer from it, do not add any comment or guess. Remain factual and avoid unnecessary embellishments, keep it simple.\n",
    "bed [0.38, 0.5, 0.74, 0.9]\n",
    "ceiling fan [0.38, 0.0, 0.64, 0.19]\n",
    "chair [0.05, 0.58, 0.27, 0.87]\n",
    "chair [0.75, 0.53, 0.81, 0.67]\n",
    "desk [0.69, 0.53, 0.89, 0.68]\n",
    "dresser [0.88, 0.6, 1.0, 1.0]\n",
    "lamp [0.25, 0.45, 0.36, 0.72]\n",
    "lamp [0.79, 0.42, 0.88, 0.54]\n",
    "plant [0.72, 0.51, 0.74, 0.53]\n",
    "plant [0.73, 0.46, 0.75, 0.53]\n",
    "side table [0.0, 0.73, 0.1, 1.0]\n",
    "side table [0.3, 0.59, 0.38, 0.73]\n",
    "side table [0.57, 0.56, 0.61, 0.58]\n",
    "window [0.14, 0.2, 0.25, 0.53]\n",
    "window [0.54, 0.37, 0.57, 0.48]\n",
    "window [0.62, 0.37, 0.64, 0.48]\n",
    "window [0.72, 0.32, 0.81, 0.51]\n",
    "window [0.91, 0.26, 1.0, 0.52]<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 2195, 2121]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are an AI model designed to help visually impaired people. Your task is to provide helpful detailed description of a scenery to guide people. \n",
      "Below is a description of a modern bedroom scene, along with a list of objects present in the scene along with their coordinates following the format 'object [x_min, y_min, x_max, y_max]'. Provide a descriptive paragraph using a human-like description, do not mention coordinates. Only use the position information and infer from it, do not add any comment or guess. Remain factual and avoid unnecessary embellishments, keep it simple.\n",
      "bed [0.38, 0.5, 0.74, 0.9]\n",
      "ceiling fan [0.38, 0.0, 0.64, 0.19]\n",
      "chair [0.05, 0.58, 0.27, 0.87]\n",
      "chair [0.75, 0.53, 0.81, 0.67]\n",
      "desk [0.69, 0.53, 0.89, 0.68]\n",
      "dresser [0.88, 0.6, 1.0, 1.0]\n",
      "lamp [0.25, 0.45, 0.36, 0.72]\n",
      "lamp [0.79, 0.42, 0.88, 0.54]\n",
      "plant [0.72, 0.51, 0.74, 0.53]\n",
      "plant [0.73, 0.46, 0.75, 0.53]\n",
      "side table [0.0, 0.73, 0.1, 1.0]\n",
      "side table [0.3, 0.59, 0.38, 0.73]\n",
      "side table [0.57, 0.56, 0.61, 0.58]\n",
      "window [0.14, 0.2, 0.25, 0.53]\n",
      "window [0.54, 0.37, 0.57, 0.48]\n",
      "window [0.62, 0.37, 0.64, 0.48]\n",
      "window [0.72, 0.32, 0.81, 0.51]\n",
      "window [0.91, 0.26, 1.0, 0.52]\n",
      "model\n",
      "This bedroom is modern in style with a bed in the center of the room. A ceiling fan is positioned above the bed, and there's a side table next to it. A dresser sits against the wall, further down the room. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question = f\"\"\"You are an AI model designed to help visually impaired people. Your task is to provide helpful detailed description of a scenery to guide people. \n",
    "# Below is a description of a {title} scene, along with a list of objects present in the scene along with their coordinates following the format 'object [x_min, y_min, x_max, y_max]'. Provide a descriptive paragraph using a human-like description, do not mention coordinates. Only use the position information and infer from it, do not add any comment or guess. Remain factual and avoid unnecessary embellishments, keep it simple.\n",
    "# {box_prompt}\"\"\"\n",
    "# prompt_phi3 = f\"<|user|>\\n{question} <|end|>\\n<|assistant|>\"\n",
    "# prompt_mistral = f\"[INST]{question}[/INST]\"\n",
    "\n",
    "# prompt = prompt_mistral\n",
    "\n",
    "inputs = tokenizer_llm(prompt, return_tensors='pt').to(device)\n",
    "#inputs = {k:v.squeeze() for k,v in inputs.items()}\n",
    "output = model_llm.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=False)\n",
    "print(tokenizer_llm.decode(output[0, :], skip_special_tokens=True))\n",
    "\n",
    "# with open(\"./promtps.txt\", 'a') as file:\n",
    "#     file.write(question + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_description",
   "language": "python",
   "name": "visual_description"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
